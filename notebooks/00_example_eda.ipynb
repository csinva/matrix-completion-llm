{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformers\n",
    "import torch\n",
    "import os.path\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from mcllm.model.llm import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data\n",
    "from mcllm.data.synthetic import LowRankDataset\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import mcllm.config\n",
    "import mcllm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader\n",
    "m = 10\n",
    "n = 10\n",
    "rank = 3\n",
    "frac_nan_mask = 0.1\n",
    "seed = 13\n",
    "use_rowcol_attn = 1\n",
    "n_registers = 0\n",
    "dataset = LowRankDataset(m, n, rank, frac_nan_mask, seed=seed,\n",
    "                         n_registers=n_registers, use_rowcol_attn=use_rowcol_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize each of these matrices as a heatmap with title as the variable name\n",
    "x_nan_t, x_clean_t, nan_mask_t, att_mask_t, register_mask_t = dataset[0]\n",
    "mat_dict = {\n",
    "    'x_nan': x_nan_t,\n",
    "    'x_clean': x_clean_t,\n",
    "    'nan_mask': nan_mask_t,\n",
    "    'att_mask': att_mask_t,\n",
    "    'register_mask': register_mask_t\n",
    "}\n",
    "for i, (k, v) in enumerate(mat_dict.items()):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    if not k == 'att_mask':\n",
    "        plt.imshow(v.reshape(m + n_registers, n + n_registers))\n",
    "    else:\n",
    "        plt.imshow(v)\n",
    "    plt.title(f'{k} mean: {v.mean().item():.2f}', fontsize='x-small')\n",
    "\n",
    "# att_mask 0\n",
    "plt.subplot(2, 3, i+1 + 1)\n",
    "plt.title('att_mask row 0', fontsize='x-small')\n",
    "plt.imshow(att_mask_t[0].reshape(m + n_registers, n + n_registers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = os.path.join(mcllm.config.path_to_repo,\n",
    "                          'results', 'attn=rowcol__reg=2__small', 'epoch=163-step=1312.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# checkpoint = os.path.join(mcllm.config.path_to_repo,\n",
    "#   'results', 'attn=rowcol__reg=2__small', 'epoch=163-step=1312.ckpt') #, 'checkpoint', 'mp_rank_00_model_states.pt')\n",
    "checkpoint = os.path.join(mcllm.config.path_to_repo,\n",
    "                          'results', 'attn=rowcol__reg=2__small', 'epoch=163-step=1312.ckpt', 'checkpoint', 'mp_rank_00_model_states.pt')\n",
    "module = mcllm.model.llm.TabLLM\n",
    "module = module.load_from_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version', 'epoch', 'global_step', 'pytorch-lightning_version', 'loops', 'callbacks', 'lr_schedulers']\n"
     ]
    }
   ],
   "source": [
    "print(list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.val_embeddings.weight',\n",
       "              tensor([[-0.9729],\n",
       "                      [-0.4033],\n",
       "                      [ 0.4942],\n",
       "                      [-0.8247],\n",
       "                      [-0.9634],\n",
       "                      [ 0.1756],\n",
       "                      [-0.5361],\n",
       "                      [ 0.9403],\n",
       "                      [ 1.1353],\n",
       "                      [-0.5213],\n",
       "                      [-0.8651],\n",
       "                      [-0.2994],\n",
       "                      [ 0.9544],\n",
       "                      [ 0.2103]], device='cuda:0')),\n",
       "             ('embedding.val_embeddings.bias',\n",
       "              tensor([ 0.0717, -0.4895, -0.5916, -0.3393,  0.5874, -0.8226,  0.8649, -0.0522,\n",
       "                      -0.1545,  0.5970,  0.9147, -0.3651, -0.1513,  0.4147], device='cuda:0')),\n",
       "             ('embedding.layer_norm.weight',\n",
       "              tensor([1.1379, 0.9941, 0.9588, 0.9586, 0.9993, 0.9572, 0.9790, 1.0478, 1.0072,\n",
       "                      0.9880, 0.9890, 0.9979, 0.9894, 1.0435, 1.0893, 1.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('embedding.layer_norm.bias',\n",
       "              tensor([ 4.0560e-02,  1.0105e-01,  4.3708e-02, -7.7918e-05, -5.8065e-03,\n",
       "                       1.6846e-01, -7.6410e-02,  8.6731e-02,  4.8130e-02, -3.5576e-02,\n",
       "                      -3.1395e-02,  4.6391e-02,  3.4825e-02, -6.0510e-02,  3.6062e-02,\n",
       "                       1.2369e-01], device='cuda:0')),\n",
       "             ('tab_layers.0.layer_norm1.weight',\n",
       "              tensor([1.1163, 1.0041, 0.9753, 0.9944, 1.0254, 0.9642, 0.9700, 1.0861, 1.0207,\n",
       "                      0.9844, 0.9908, 1.0005, 1.0001, 1.0415, 1.0440, 1.0976],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.layer_norm1.bias',\n",
       "              tensor([ 0.0022,  0.0037,  0.0073,  0.0117,  0.0269,  0.0067, -0.0026,  0.0085,\n",
       "                      -0.0057, -0.0018, -0.0095, -0.0105, -0.0358,  0.0006,  0.0036, -0.0049],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.query.weight',\n",
       "              tensor([[-0.0648, -0.1875, -0.2876, -0.1812,  0.1070,  0.1135,  0.2886, -0.1718,\n",
       "                       -0.0979, -0.0502,  0.1663, -0.2192,  0.1959, -0.1755,  0.1657,  0.0054],\n",
       "                      [ 0.2169,  0.1107,  0.1660, -0.1054, -0.1173,  0.1393,  0.1194, -0.0073,\n",
       "                       -0.1999,  0.2588,  0.1364, -0.0225, -0.1622,  0.0991, -0.2032,  0.1338]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.query.bias',\n",
       "              tensor([-0.1553, -0.0113], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.key.weight',\n",
       "              tensor([[-0.0895,  0.0185, -0.1705, -0.2636, -0.1067,  0.0886,  0.1109,  0.2862,\n",
       "                        0.1147, -0.2803, -0.2951, -0.0245, -0.0843,  0.0136, -0.1658, -0.1789],\n",
       "                      [-0.1759,  0.0402,  0.1755, -0.1068, -0.2127,  0.2500, -0.1452,  0.1013,\n",
       "                        0.0098,  0.2134, -0.0780,  0.0962,  0.0959,  0.1462, -0.3316,  0.1474]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.key.bias',\n",
       "              tensor([ 0.1342, -0.2064], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.values.weight',\n",
       "              tensor([[ 0.1057,  0.0574, -0.0613, -0.0227,  0.0734, -0.2212, -0.0708, -0.1859,\n",
       "                       -0.0888, -0.1436, -0.1025,  0.1848,  0.1799,  0.1714,  0.1844, -0.1773],\n",
       "                      [-0.1417,  0.0167, -0.1991,  0.2608,  0.1762,  0.0655,  0.0593, -0.1637,\n",
       "                       -0.0417,  0.0114,  0.0536,  0.0737,  0.1968, -0.1306,  0.1117, -0.0314]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.0.values.bias',\n",
       "              tensor([-0.2045,  0.1051], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.2681,  0.2996,  0.1162,  0.0250, -0.1509,  0.2337, -0.2388, -0.0932,\n",
       "                        0.2674, -0.0595,  0.0748,  0.0863, -0.0678, -0.0327,  0.0624,  0.0449],\n",
       "                      [ 0.1510, -0.0434, -0.2184,  0.1107, -0.0806, -0.1818,  0.1754,  0.1036,\n",
       "                        0.0043, -0.1456, -0.0457,  0.1884, -0.0900,  0.0846, -0.0224,  0.1786]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.query.bias',\n",
       "              tensor([-0.2723,  0.0624], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.key.weight',\n",
       "              tensor([[-0.2910,  0.2475,  0.1790,  0.1754,  0.0310,  0.2618,  0.2378,  0.0309,\n",
       "                        0.2430,  0.0411, -0.2207,  0.1319,  0.1213,  0.1664,  0.1123, -0.0118],\n",
       "                      [-0.1171,  0.0644, -0.1318,  0.0892,  0.2628,  0.2110,  0.0466, -0.0628,\n",
       "                       -0.0605,  0.2145,  0.1397, -0.0284,  0.1844,  0.0037,  0.1811, -0.1408]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.key.bias',\n",
       "              tensor([-0.0324,  0.2401], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.values.weight',\n",
       "              tensor([[ 2.8297e-01,  5.9426e-02, -7.4724e-02, -9.7838e-02,  2.9663e-02,\n",
       "                        4.7268e-02, -2.0067e-01,  9.2370e-02,  1.3453e-01,  2.1837e-01,\n",
       "                       -3.5825e-03, -2.2979e-01, -6.7169e-02,  1.8960e-04, -3.3881e-02,\n",
       "                        2.1105e-01],\n",
       "                      [-1.8650e-01, -1.3510e-02, -1.7600e-01, -9.4061e-03, -1.1352e-01,\n",
       "                       -1.5411e-01,  1.8026e-01,  1.4369e-01, -1.6220e-01, -2.4406e-01,\n",
       "                        1.4519e-01,  5.5940e-02,  6.3584e-02,  1.1715e-01,  6.7515e-04,\n",
       "                        1.6187e-01]], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.1.values.bias',\n",
       "              tensor([0.0812, 0.1551], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.query.weight',\n",
       "              tensor([[-0.1776, -0.1910, -0.0501,  0.1923, -0.0527,  0.2940,  0.1531,  0.1610,\n",
       "                        0.1923,  0.0811, -0.0267,  0.1683,  0.0859, -0.2560,  0.0126,  0.1650],\n",
       "                      [-0.2179,  0.2040,  0.0784,  0.1790, -0.3316,  0.1468, -0.1104,  0.3328,\n",
       "                        0.4361, -0.1467, -0.0680,  0.0543, -0.0666, -0.1572, -0.1458, -0.2188]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.query.bias',\n",
       "              tensor([-0.2288, -0.1968], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.key.weight',\n",
       "              tensor([[-0.0960,  0.0053, -0.1263,  0.2569,  0.2114, -0.1068,  0.0797,  0.0630,\n",
       "                       -0.0591,  0.2026, -0.0296, -0.1763, -0.2215,  0.0487, -0.0146, -0.1846],\n",
       "                      [-0.0659, -0.0932, -0.1251, -0.0072,  0.2278, -0.2015,  0.0356, -0.0399,\n",
       "                       -0.2008,  0.2009,  0.2070, -0.1004,  0.1451,  0.0509,  0.4117, -0.1652]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.key.bias',\n",
       "              tensor([ 0.1824, -0.0891], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.values.weight',\n",
       "              tensor([[ 0.2787, -0.2336,  0.0017,  0.0745,  0.1858, -0.2212, -0.2337, -0.1971,\n",
       "                        0.0804,  0.0294, -0.2036, -0.0273,  0.0977, -0.2055, -0.1617,  0.2406],\n",
       "                      [-0.2364, -0.1830, -0.0192, -0.2942, -0.1808, -0.2101, -0.0322,  0.0924,\n",
       "                        0.0068,  0.1271, -0.1923,  0.1312,  0.2957, -0.1389, -0.0618,  0.2509]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.2.values.bias',\n",
       "              tensor([0.1643, 0.2504], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.query.weight',\n",
       "              tensor([[-0.3296, -0.1479,  0.0994,  0.1587,  0.2729, -0.1159,  0.0171, -0.1311,\n",
       "                       -0.1473,  0.0105,  0.0487,  0.2155,  0.0302,  0.2261,  0.2748,  0.0286],\n",
       "                      [-0.3234, -0.1961, -0.0122, -0.1445, -0.1536, -0.0977,  0.2812, -0.0519,\n",
       "                       -0.1490, -0.1995,  0.1719, -0.0480, -0.1942, -0.1729,  0.1235,  0.0528]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.query.bias',\n",
       "              tensor([ 0.2636, -0.1040], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.key.weight',\n",
       "              tensor([[ 0.0123, -0.1116,  0.0193, -0.2618,  0.0769,  0.2092,  0.1542,  0.2295,\n",
       "                        0.2117, -0.0055, -0.1005,  0.2092, -0.0051, -0.1507, -0.1060,  0.0291],\n",
       "                      [ 0.1755, -0.0220,  0.1452, -0.0784,  0.0722,  0.2280, -0.0856,  0.1513,\n",
       "                        0.0031, -0.0702, -0.0252,  0.0050,  0.1556, -0.3204,  0.0368,  0.2074]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.key.bias',\n",
       "              tensor([-0.1303,  0.1141], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.values.weight',\n",
       "              tensor([[-0.1963, -0.0245, -0.0073,  0.0122, -0.1547,  0.1078,  0.1946, -0.1279,\n",
       "                        0.1915,  0.0886, -0.2242, -0.0290, -0.0966,  0.1551, -0.2160, -0.1605],\n",
       "                      [-0.2585, -0.1705,  0.1229, -0.0841,  0.2332,  0.1928, -0.1631, -0.1736,\n",
       "                        0.1294, -0.0944, -0.1470, -0.0347, -0.1803,  0.0522, -0.2218,  0.2718]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.3.values.bias',\n",
       "              tensor([0.2060, 0.2370], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 0.0945, -0.3135, -0.1283, -0.0484,  0.3996, -0.2726,  0.0862, -0.3124,\n",
       "                       -0.4162,  0.3256,  0.2899, -0.0358,  0.0239,  0.2561,  0.1464, -0.2478],\n",
       "                      [-0.1640,  0.1792, -0.1768,  0.0409, -0.1247, -0.1214, -0.0296,  0.3203,\n",
       "                        0.3098, -0.0909,  0.1047, -0.1342, -0.0573, -0.0498, -0.1240,  0.2864]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.query.bias',\n",
       "              tensor([ 0.0344, -0.2293], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.key.weight',\n",
       "              tensor([[ 0.1980,  0.2134,  0.0304, -0.0889, -0.0522,  0.2334, -0.2244, -0.1035,\n",
       "                       -0.3075, -0.0039, -0.1154,  0.2935,  0.1461, -0.0047, -0.3172,  0.1812],\n",
       "                      [-0.1041,  0.1736, -0.0243,  0.0148,  0.0793,  0.1604, -0.0236, -0.0616,\n",
       "                        0.0934,  0.0543,  0.1428,  0.1870,  0.1180, -0.0179,  0.1611, -0.2683]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.key.bias',\n",
       "              tensor([-0.1882, -0.1481], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.values.weight',\n",
       "              tensor([[-0.2282,  0.0902,  0.2111, -0.1161,  0.1022,  0.0602, -0.0464,  0.0008,\n",
       "                        0.0051,  0.1049, -0.1065,  0.0274,  0.0660,  0.1840, -0.1764,  0.2794],\n",
       "                      [ 0.2594,  0.1380, -0.0430, -0.1904,  0.0188,  0.1998,  0.1163,  0.0818,\n",
       "                       -0.2624, -0.1743, -0.0473, -0.0761,  0.0868,  0.0020,  0.1196, -0.2162]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.4.values.bias',\n",
       "              tensor([-0.0778, -0.2056], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.0374, -0.1664, -0.3567, -0.2309, -0.0169, -0.0702,  0.3903, -0.2914,\n",
       "                       -0.1191,  0.0504, -0.0085, -0.3450, -0.1562,  0.3439,  0.1394,  0.1514],\n",
       "                      [-0.0814,  0.4498,  0.2902,  0.4352, -0.2061,  0.2414, -0.0139,  0.3822,\n",
       "                        0.0679, -0.0189, -0.3727,  0.3690,  0.1297, -0.1123, -0.2101, -0.1937]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.query.bias',\n",
       "              tensor([ 0.0374, -0.3042], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.key.weight',\n",
       "              tensor([[ 0.0795,  0.4588,  0.1560, -0.0295,  0.2813,  0.3306,  0.0867,  0.1023,\n",
       "                       -0.1801, -0.0314,  0.1319,  0.5173, -0.2417, -0.4181, -0.2625,  0.0285],\n",
       "                      [ 0.0540, -0.2863, -0.0175, -0.1020, -0.0069,  0.0684,  0.3780, -0.1053,\n",
       "                        0.3501, -0.0570,  0.0807, -0.3012,  0.1998,  0.4544, -0.0964,  0.2092]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.key.bias',\n",
       "              tensor([0.1704, 0.1297], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.values.weight',\n",
       "              tensor([[ 0.1116, -0.1989, -0.1234, -0.1875,  0.1496, -0.1261, -0.1863,  0.0688,\n",
       "                        0.2598,  0.0024,  0.1829,  0.1476,  0.1725, -0.1617, -0.1382,  0.0658],\n",
       "                      [ 0.0043,  0.2674, -0.0847,  0.2106,  0.1696,  0.1744,  0.1413, -0.0351,\n",
       "                       -0.0836, -0.0185,  0.2224,  0.2210, -0.3129, -0.1366,  0.0072,  0.0905]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.5.values.bias',\n",
       "              tensor([0.2280, 0.2428], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.query.weight',\n",
       "              tensor([[ 0.3319,  0.1084, -0.4102, -0.2568,  0.2174, -0.3109,  0.0260, -0.2549,\n",
       "                       -0.3322,  0.1371,  0.0878,  0.1493, -0.0468, -0.0343,  0.1110,  0.2457],\n",
       "                      [ 0.0187, -0.3474, -0.1765, -0.0696,  0.2182, -0.1825,  0.3357,  0.0242,\n",
       "                       -0.4020,  0.2882,  0.3343, -0.1895, -0.2225, -0.0431,  0.2739,  0.1313]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.query.bias',\n",
       "              tensor([-0.0490,  0.3250], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.key.weight',\n",
       "              tensor([[-0.1271, -0.0520,  0.1265, -0.0450, -0.1869,  0.1572, -0.2440, -0.0437,\n",
       "                        0.0522, -0.3964, -0.2220,  0.2152,  0.2686,  0.0950,  0.2668,  0.0289],\n",
       "                      [ 0.0450,  0.2791,  0.1577,  0.0885, -0.2621,  0.2277, -0.3666,  0.0652,\n",
       "                        0.0424, -0.3049, -0.3527,  0.2804,  0.1182, -0.1548, -0.0660, -0.2456]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.key.bias',\n",
       "              tensor([0.1227, 0.1665], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.values.weight',\n",
       "              tensor([[-0.4610, -0.1245,  0.2755, -0.0771, -0.2898,  0.2193, -0.1822,  0.4630,\n",
       "                        0.0885, -0.1690, -0.0924, -0.1938,  0.1454,  0.1083, -0.0056,  0.0912],\n",
       "                      [ 0.3630, -0.2115,  0.0809,  0.1052, -0.2293, -0.0394,  0.0905,  0.0363,\n",
       "                        0.0636,  0.0311,  0.2185, -0.0937,  0.0240,  0.1026,  0.2041, -0.1403]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.6.values.bias',\n",
       "              tensor([ 0.0872, -0.0667], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.query.weight',\n",
       "              tensor([[ 0.1960,  0.0512, -0.0981,  0.2809,  0.1902,  0.0940,  0.1400, -0.3473,\n",
       "                       -0.1544, -0.2603, -0.2024,  0.1590, -0.1759, -0.1114, -0.0458, -0.1343],\n",
       "                      [ 0.4213,  0.1673, -0.1520,  0.4564,  0.0838, -0.1505, -0.0603, -0.4069,\n",
       "                       -0.2502, -0.0213, -0.0447, -0.1913, -0.2117, -0.0151,  0.2716, -0.3358]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.query.bias',\n",
       "              tensor([-0.0522, -0.2359], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.key.weight',\n",
       "              tensor([[ 0.3877,  0.1105, -0.2114,  0.2110,  0.0939,  0.1049,  0.2472, -0.0909,\n",
       "                       -0.2239,  0.0545,  0.2086,  0.1654,  0.0171,  0.0210,  0.1235,  0.2394],\n",
       "                      [ 0.2924,  0.3847, -0.0227,  0.2514,  0.1526,  0.1617,  0.1957, -0.0987,\n",
       "                       -0.4727, -0.0109,  0.2421,  0.1740, -0.4044, -0.2042,  0.0218, -0.1737]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.key.bias',\n",
       "              tensor([-0.2445, -0.0751], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.values.weight',\n",
       "              tensor([[ 0.1046,  0.1246,  0.0618, -0.1123,  0.1181,  0.1511, -0.2536, -0.2985,\n",
       "                       -0.2646,  0.1433,  0.1095, -0.1146, -0.1860, -0.1471,  0.0858,  0.2853],\n",
       "                      [ 0.1095, -0.0375, -0.1815, -0.1878,  0.0378, -0.0136, -0.2448, -0.1461,\n",
       "                       -0.1900, -0.1614,  0.1367,  0.1130,  0.2192, -0.0717, -0.1050, -0.3185]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.heads.7.values.bias',\n",
       "              tensor([-0.2082, -0.0891], device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.proj.weight',\n",
       "              tensor([[-0.0419, -0.0777, -0.1793, -0.0665, -0.1323,  0.2437,  0.2564, -0.0894,\n",
       "                        0.1409,  0.0681, -0.0403, -0.2578,  0.2125,  0.0942, -0.2922, -0.0947],\n",
       "                      [-0.0973, -0.0867, -0.0223, -0.1642,  0.1673,  0.1387, -0.1448, -0.0923,\n",
       "                       -0.0052,  0.0618, -0.0418,  0.1236,  0.1725, -0.1753,  0.4447,  0.1569],\n",
       "                      [-0.2489, -0.1603,  0.0942,  0.1436,  0.2645, -0.0790, -0.1391,  0.0422,\n",
       "                       -0.0796,  0.1367, -0.2099, -0.0034,  0.3247,  0.0618,  0.5125, -0.2544],\n",
       "                      [ 0.0775, -0.0872,  0.2591,  0.2628,  0.0178,  0.1977,  0.1296,  0.2324,\n",
       "                        0.0589, -0.2184,  0.0360,  0.0936, -0.1109, -0.1689, -0.0932, -0.2239],\n",
       "                      [-0.2114,  0.1432, -0.0065,  0.0799,  0.0952,  0.0249, -0.1198, -0.1600,\n",
       "                       -0.0449,  0.1985,  0.0094, -0.0298, -0.1167,  0.1638, -0.1540,  0.1946],\n",
       "                      [-0.2995,  0.1141,  0.2187, -0.0904,  0.2952,  0.0867, -0.1277,  0.3966,\n",
       "                        0.3746, -0.1496,  0.2441, -0.2080,  0.1261, -0.1188,  0.3720,  0.0626],\n",
       "                      [ 0.0494, -0.1291,  0.0353, -0.1245,  0.0287,  0.1613,  0.2771,  0.0593,\n",
       "                       -0.1231,  0.2238, -0.0772, -0.1508,  0.1277, -0.1040, -0.0721,  0.0337],\n",
       "                      [ 0.0767,  0.1726,  0.1882,  0.2433, -0.0930,  0.2963,  0.0637,  0.1766,\n",
       "                       -0.1470, -0.0619,  0.0328, -0.1489,  0.1246, -0.1533,  0.2332, -0.1565],\n",
       "                      [-0.0650,  0.1979,  0.2062,  0.1481,  0.1195,  0.1955, -0.0987,  0.2628,\n",
       "                        0.1144, -0.0971, -0.1177,  0.2241,  0.2130, -0.0091,  0.2038, -0.0519],\n",
       "                      [ 0.2030,  0.0247,  0.1952, -0.0721,  0.1213, -0.2129,  0.1163,  0.1737,\n",
       "                       -0.1190, -0.0896,  0.2007, -0.2032, -0.1691,  0.0682, -0.1481, -0.1615],\n",
       "                      [-0.1932,  0.0428,  0.1495,  0.2191, -0.0935,  0.0525,  0.2902, -0.2297,\n",
       "                        0.2070,  0.0541,  0.0297, -0.0965,  0.0057,  0.2234, -0.4165,  0.2697],\n",
       "                      [ 0.0178,  0.1154, -0.0104, -0.1479, -0.0625, -0.1150,  0.0669, -0.1442,\n",
       "                        0.1039, -0.0391, -0.0094, -0.1544, -0.1390,  0.0037,  0.2814,  0.0611],\n",
       "                      [-0.0951,  0.0602, -0.2340,  0.0785, -0.1337, -0.3131, -0.0710, -0.1845,\n",
       "                       -0.0657,  0.0656, -0.0194, -0.1560, -0.3073,  0.1102,  0.2382,  0.0647],\n",
       "                      [-0.1788, -0.0652, -0.1060,  0.1642,  0.1823,  0.1937,  0.3385,  0.0695,\n",
       "                       -0.0040, -0.0053,  0.2493,  0.1999, -0.0575, -0.2418, -0.1238,  0.1153],\n",
       "                      [ 0.1922,  0.2266,  0.1991, -0.0785,  0.0523, -0.1867, -0.1458, -0.1024,\n",
       "                        0.0420,  0.1797, -0.1246, -0.1612, -0.2845, -0.0113,  0.0034, -0.1247],\n",
       "                      [-0.0767,  0.1097,  0.1587, -0.0554, -0.0532, -0.0143, -0.2663, -0.0264,\n",
       "                       -0.4426,  0.2098, -0.2386,  0.2047, -0.1008, -0.0622, -0.0627,  0.2865]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.self_attention.proj.bias',\n",
       "              tensor([-0.1839,  0.0435,  0.0003,  0.0683,  0.2648,  0.1014, -0.2082,  0.1348,\n",
       "                      -0.0548, -0.2431,  0.1806,  0.1450, -0.2721, -0.1628,  0.2254, -0.1305],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.layer_norm2.weight',\n",
       "              tensor([1.0378, 0.9898, 0.9738, 0.9815, 1.0054, 0.9535, 0.9304, 1.0663, 0.9978,\n",
       "                      0.9777, 0.9851, 1.0029, 1.0329, 1.0165, 1.0371, 1.0498],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.layer_norm2.bias',\n",
       "              tensor([ 0.0047,  0.0117,  0.0083,  0.0136,  0.0214,  0.0002,  0.0042,  0.0080,\n",
       "                      -0.0045, -0.0006, -0.0052, -0.0085, -0.0197,  0.0049,  0.0045, -0.0148],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.mlp.mlp.0.weight',\n",
       "              tensor([[ 0.2490,  0.2144,  0.1655,  ...,  0.1471,  0.1887,  0.0200],\n",
       "                      [-0.1018,  0.2214, -0.2156,  ..., -0.1510, -0.2459,  0.1503],\n",
       "                      [-0.0823,  0.1012, -0.2300,  ...,  0.1743, -0.1886,  0.2198],\n",
       "                      ...,\n",
       "                      [-0.0305, -0.0678, -0.2215,  ...,  0.0663,  0.2153, -0.1757],\n",
       "                      [ 0.1897, -0.1456,  0.1331,  ..., -0.0195,  0.1591,  0.0523],\n",
       "                      [ 0.2265, -0.1971,  0.1614,  ...,  0.0036, -0.1331, -0.2359]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.mlp.mlp.0.bias',\n",
       "              tensor([ 1.7988e-01,  2.3633e-01,  3.7952e-02, -6.1718e-02,  1.3677e-01,\n",
       "                       1.6507e-01, -1.0599e-01,  1.1510e-01,  2.8031e-01,  4.2951e-02,\n",
       "                      -7.0541e-02, -8.9456e-02, -1.8019e-01,  7.6647e-02, -2.1702e-01,\n",
       "                      -1.0853e-01,  1.8071e-02, -1.0909e-01,  4.4007e-02, -2.0060e-01,\n",
       "                       1.6382e-01,  1.4923e-01, -1.9297e-02,  9.3416e-02,  1.8984e-01,\n",
       "                       2.1652e-01,  2.5652e-01, -3.0308e-02, -3.9615e-02, -3.2759e-02,\n",
       "                      -2.1392e-01,  1.3808e-01,  2.4475e-02, -1.1533e-01, -7.7110e-02,\n",
       "                      -1.4084e-01, -1.5634e-01, -1.7031e-01, -2.2819e-01, -2.2743e-02,\n",
       "                       1.1141e-01, -1.4335e-02,  1.3562e-01, -8.2349e-02,  5.5187e-02,\n",
       "                      -2.2582e-05, -8.7919e-02,  1.5278e-01, -9.8917e-02,  9.9104e-02,\n",
       "                      -2.5029e-02, -1.3032e-01,  1.1450e-01, -1.6542e-01,  1.7507e-01,\n",
       "                      -1.0498e-01, -1.4460e-01, -2.1704e-01, -7.1342e-02,  6.2175e-02,\n",
       "                       2.0174e-02,  1.0930e-01,  2.5775e-01,  1.7006e-01], device='cuda:0')),\n",
       "             ('tab_layers.0.mlp.mlp.2.weight',\n",
       "              tensor([[ 0.1269, -0.0873,  0.1184,  ...,  0.0272,  0.1086,  0.0491],\n",
       "                      [ 0.1415, -0.0849, -0.1046,  ..., -0.1367,  0.0467, -0.0139],\n",
       "                      [ 0.1358,  0.0704,  0.0378,  ..., -0.0779, -0.0933,  0.2039],\n",
       "                      ...,\n",
       "                      [-0.0852, -0.1026,  0.1417,  ...,  0.1691,  0.0204,  0.0218],\n",
       "                      [ 0.0173,  0.0578,  0.0663,  ..., -0.0519, -0.0612,  0.0297],\n",
       "                      [-0.1161,  0.0908,  0.1105,  ..., -0.0236, -0.0831,  0.0529]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.0.mlp.mlp.2.bias',\n",
       "              tensor([-0.1083, -0.0931, -0.1093,  0.0406, -0.0572, -0.1188, -0.1204,  0.1034,\n",
       "                       0.0291,  0.0383,  0.0449, -0.0852,  0.0350, -0.0917, -0.0171,  0.0767],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.layer_norm1.weight',\n",
       "              tensor([1.0631, 0.9898, 1.0277, 1.0682, 0.9933, 0.9688, 0.9365, 1.0617, 1.0364,\n",
       "                      0.9920, 1.0073, 1.0037, 1.0353, 1.0456, 1.0287, 1.0497],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.layer_norm1.bias',\n",
       "              tensor([-1.2348e-02,  4.4577e-03, -1.0944e-02,  1.3188e-02,  1.0603e-02,\n",
       "                       4.0197e-03, -2.8660e-03,  1.7262e-02, -2.0720e-03, -6.4742e-05,\n",
       "                       8.7240e-03, -1.0785e-02, -9.8594e-03,  1.2461e-02,  1.1478e-02,\n",
       "                      -8.6675e-03], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.query.weight',\n",
       "              tensor([[-0.1216,  0.3320,  0.0097, -0.5361, -0.5440,  0.0688, -0.2192,  0.1719,\n",
       "                        0.6002, -0.4417, -0.3623,  0.5975,  0.3829, -0.0710,  0.1677, -0.1988],\n",
       "                      [ 0.1716, -0.1892, -0.3022,  0.3073,  0.1392, -0.0107,  0.2010, -0.0845,\n",
       "                       -0.5636,  0.2010,  0.5389, -0.6012, -0.4239,  0.0674,  0.0271,  0.2227]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.query.bias',\n",
       "              tensor([-0.2326,  0.2295], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.key.weight',\n",
       "              tensor([[-0.2427, -0.6223, -0.4666, -0.2808, -0.0483, -0.4514,  0.3009,  0.3214,\n",
       "                        0.2911,  0.1976, -0.0901,  0.0519, -0.0670,  0.1670,  0.2011, -0.1163],\n",
       "                      [ 0.3598,  0.6706,  0.2847,  0.1820,  0.0346,  0.5416, -0.1863, -0.3852,\n",
       "                       -0.2143, -0.1729, -0.1521,  0.1385,  0.1008, -0.2894, -0.3235,  0.0550]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.key.bias',\n",
       "              tensor([ 0.0554, -0.0653], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.values.weight',\n",
       "              tensor([[-0.1967,  0.1855, -0.1855, -0.1191, -0.0320,  0.0134, -0.0666, -0.1487,\n",
       "                       -0.1551, -0.1584, -0.1403,  0.0788,  0.1267, -0.0875, -0.1140, -0.2539],\n",
       "                      [ 0.1286,  0.1997, -0.0652, -0.0544,  0.0008,  0.0477, -0.1015, -0.0007,\n",
       "                        0.1875, -0.0888, -0.2066,  0.1112,  0.1753, -0.1587, -0.0119,  0.2793]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.0.values.bias',\n",
       "              tensor([-0.1877,  0.0598], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.1668, -0.0689,  0.1147,  0.0660,  0.2484,  0.1900,  0.2171,  0.1174,\n",
       "                        0.0013, -0.0522,  0.2504,  0.2071,  0.0988, -0.1589,  0.0548,  0.2644],\n",
       "                      [ 0.0979, -0.2935,  0.0279, -0.0168, -0.1708, -0.1864,  0.0599,  0.0151,\n",
       "                        0.3748, -0.0984,  0.1663, -0.0009,  0.3004,  0.1725, -0.1006,  0.0437]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.query.bias',\n",
       "              tensor([ 0.2288, -0.2072], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.key.weight',\n",
       "              tensor([[ 0.1598,  0.0104, -0.0110, -0.1047, -0.1825,  0.1699, -0.0635,  0.2892,\n",
       "                       -0.0879,  0.2200,  0.0134, -0.1841,  0.1498, -0.1173, -0.2582,  0.0525],\n",
       "                      [-0.2137, -0.1383,  0.2545,  0.0371, -0.1962,  0.2100, -0.1456,  0.2566,\n",
       "                        0.0600, -0.2695,  0.0090,  0.3315,  0.3199, -0.1831, -0.1176, -0.1277]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.key.bias',\n",
       "              tensor([0.1722, 0.0244], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.values.weight',\n",
       "              tensor([[ 0.1655, -0.0638,  0.0017, -0.0815, -0.2230, -0.1073, -0.0067,  0.2317,\n",
       "                        0.0809,  0.0149,  0.2091, -0.0644, -0.0986,  0.0353, -0.1938,  0.2682],\n",
       "                      [-0.2077,  0.0504,  0.0540, -0.0558,  0.1072,  0.0513,  0.1171, -0.0945,\n",
       "                       -0.1067, -0.2121, -0.0170, -0.1895, -0.1234, -0.1662,  0.3001, -0.0991]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.1.values.bias',\n",
       "              tensor([-0.0579, -0.2365], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.query.weight',\n",
       "              tensor([[-0.4280, -0.2410, -0.1639,  0.1048,  0.1631,  0.0769,  0.2852,  0.3865,\n",
       "                        0.0545,  0.3448,  0.3827, -0.3963, -0.3422,  0.1971,  0.3181, -0.1955],\n",
       "                      [ 0.2629,  0.2337,  0.1308, -0.0577,  0.0572,  0.1629, -0.1874, -0.4145,\n",
       "                       -0.0020, -0.0998, -0.1159,  0.0885,  0.2655, -0.1436, -0.0951, -0.2503]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.query.bias',\n",
       "              tensor([ 0.3514, -0.3181], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.key.weight',\n",
       "              tensor([[-0.0625,  0.0382,  0.4273, -0.0159, -0.1684,  0.3612, -0.1243,  0.2300,\n",
       "                        0.1761, -0.1926, -0.0772,  0.0924, -0.1138, -0.1467, -0.1225, -0.3943],\n",
       "                      [ 0.2887,  0.0475, -0.4064,  0.0353,  0.2743, -0.0341,  0.2755,  0.0274,\n",
       "                       -0.1051,  0.2082,  0.1269, -0.3376,  0.1430,  0.2841,  0.2064,  0.2270]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.key.bias',\n",
       "              tensor([ 0.0463, -0.1312], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.values.weight',\n",
       "              tensor([[-0.1937,  0.0970,  0.1191, -0.0186,  0.1723,  0.0766, -0.0101,  0.1813,\n",
       "                        0.0460, -0.0884, -0.2507,  0.2507,  0.0692, -0.0808,  0.0162, -0.0443],\n",
       "                      [ 0.2606, -0.0330,  0.1033,  0.0786,  0.0672, -0.2034,  0.1697,  0.0839,\n",
       "                       -0.2199, -0.1909,  0.2475, -0.2387, -0.3767, -0.0483, -0.3264, -0.0082]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.2.values.bias',\n",
       "              tensor([-0.2283, -0.1225], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.query.weight',\n",
       "              tensor([[-0.2422, -0.2226, -0.4470,  0.3743,  0.4344, -0.3617, -0.0107,  0.2355,\n",
       "                       -0.2222,  0.2338,  0.4224, -0.2981, -0.0057,  0.1029,  0.0944,  0.0679],\n",
       "                      [-0.4664, -0.4217, -0.1456,  0.1046,  0.3194, -0.1405, -0.0636,  0.5906,\n",
       "                        0.2515,  0.1051,  0.0360, -0.3944, -0.1270,  0.1106,  0.0257,  0.2518]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.query.bias',\n",
       "              tensor([0.4686, 0.0399], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.key.weight',\n",
       "              tensor([[ 0.1216,  0.2649,  0.3778,  0.2112, -0.0344,  0.2242, -0.1653,  0.0181,\n",
       "                        0.1141, -0.4496, -0.4468,  0.3631, -0.0889, -0.0180,  0.1349, -0.1890],\n",
       "                      [ 0.1216, -0.0982,  0.2318, -0.1098, -0.2138,  0.3630, -0.0624,  0.1427,\n",
       "                       -0.1128, -0.3533, -0.2464,  0.0680, -0.0674, -0.2155,  0.1689,  0.1423]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.key.bias',\n",
       "              tensor([0.0829, 0.2699], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.values.weight',\n",
       "              tensor([[-0.1376, -0.1865,  0.1889,  0.0119, -0.0780, -0.0946,  0.1031, -0.0996,\n",
       "                       -0.2369,  0.1165,  0.0886, -0.0482,  0.2423, -0.0871, -0.2233,  0.1189],\n",
       "                      [-0.1924, -0.1790,  0.1579, -0.2104, -0.2409, -0.1237, -0.0490, -0.0458,\n",
       "                        0.2434,  0.0594, -0.2226,  0.1009,  0.0695,  0.2049,  0.1319, -0.0120]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.3.values.bias',\n",
       "              tensor([0.0052, 0.1786], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 2.7757e-01,  2.7126e-02, -3.1575e-01,  2.9911e-01,  2.6724e-01,\n",
       "                       -9.5859e-02, -5.8689e-04, -2.6352e-01, -6.4214e-01,  3.1348e-01,\n",
       "                        1.5512e-01, -2.4171e-02,  2.3672e-03,  1.7768e-01,  6.4097e-02,\n",
       "                        7.7601e-02],\n",
       "                      [ 2.5474e-01, -1.2264e-01,  1.8211e-01, -9.1522e-02, -1.8445e-01,\n",
       "                        3.5986e-01, -4.7862e-01,  4.5790e-02,  2.0256e-01, -5.4883e-02,\n",
       "                       -3.5924e-01,  2.2764e-01,  4.3639e-01, -1.1869e-01, -5.6094e-02,\n",
       "                       -1.3134e-01]], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.query.bias',\n",
       "              tensor([ 0.2415, -0.3557], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.key.weight',\n",
       "              tensor([[ 0.0923, -0.0062,  0.1057,  0.4087, -0.2092, -0.0939, -0.1708, -0.2368,\n",
       "                       -0.2884,  0.0443,  0.0009, -0.0862, -0.3544,  0.0020, -0.0078,  0.1886],\n",
       "                      [ 0.0452, -0.1267,  0.3231, -0.3286,  0.1799, -0.0946, -0.0080,  0.1488,\n",
       "                        0.2948, -0.3390, -0.2038, -0.1127,  0.2023, -0.4032,  0.2739, -0.2263]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.key.bias',\n",
       "              tensor([ 0.2038, -0.0697], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.values.weight',\n",
       "              tensor([[-0.1739,  0.2573,  0.0743,  0.0175, -0.0109,  0.1678, -0.0647, -0.0955,\n",
       "                       -0.1634,  0.1951,  0.1673,  0.1434,  0.0713,  0.0244, -0.1961,  0.2686],\n",
       "                      [-0.1833, -0.1408,  0.1910, -0.0821,  0.2475, -0.0293, -0.0009,  0.1822,\n",
       "                       -0.2167, -0.1745, -0.0665,  0.1461,  0.1366, -0.1983, -0.1363,  0.3120]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.4.values.bias',\n",
       "              tensor([ 0.0842, -0.0923], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.3784, -0.1579, -0.1862, -0.2952, -0.0139,  0.1006, -0.0529,  0.3779,\n",
       "                        0.0521,  0.2216,  0.1840, -0.1704,  0.0261,  0.1943,  0.1331, -0.0411],\n",
       "                      [-0.1905,  0.0356,  0.0792,  0.0066, -0.0316, -0.2693,  0.1185, -0.1890,\n",
       "                        0.0327, -0.2102, -0.0077, -0.1599,  0.0176, -0.0102,  0.1587,  0.1541]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.query.bias',\n",
       "              tensor([0.3062, 0.2784], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.key.weight',\n",
       "              tensor([[-0.1827,  0.3791,  0.0724, -0.0747, -0.1221,  0.3136, -0.1641, -0.0414,\n",
       "                       -0.0676, -0.2487,  0.0037,  0.2294, -0.1209, -0.2203,  0.2663, -0.1205],\n",
       "                      [-0.0532, -0.0757,  0.1173,  0.0212, -0.3230,  0.1769,  0.0982,  0.0317,\n",
       "                        0.0413, -0.2591, -0.1183,  0.1750, -0.0421,  0.0044,  0.2083,  0.2030]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.key.bias',\n",
       "              tensor([ 0.2089, -0.0871], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.values.weight',\n",
       "              tensor([[-0.1078, -0.1556,  0.2289, -0.2090, -0.1759, -0.0603, -0.1565, -0.1136,\n",
       "                        0.1182,  0.0044,  0.2090,  0.0900, -0.0733,  0.2099, -0.2169, -0.1899],\n",
       "                      [ 0.2211,  0.2081, -0.0050, -0.0976,  0.1678,  0.1455,  0.1955,  0.0826,\n",
       "                        0.1383,  0.1431, -0.0152,  0.0909, -0.0545, -0.1522, -0.2440, -0.1919]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.5.values.bias',\n",
       "              tensor([0.1780, 0.1294], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.query.weight',\n",
       "              tensor([[ 0.0321, -0.2260,  0.0465,  0.0988, -0.0225, -0.1648,  0.1160,  0.0692,\n",
       "                       -0.1321, -0.0209,  0.1800,  0.0560, -0.1642,  0.1499, -0.1489,  0.0159],\n",
       "                      [ 0.1230, -0.1821,  0.0336, -0.0480,  0.0591, -0.0712,  0.1244,  0.0544,\n",
       "                        0.0807,  0.0246, -0.2573, -0.0679, -0.1712, -0.0436, -0.2610,  0.1609]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.query.bias',\n",
       "              tensor([-0.0568, -0.0691], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.key.weight',\n",
       "              tensor([[ 0.1999, -0.0067, -0.1774,  0.1685,  0.1467, -0.1283,  0.0045, -0.0793,\n",
       "                        0.2025, -0.0561,  0.0902, -0.1614,  0.1200,  0.3264,  0.1976,  0.1401],\n",
       "                      [-0.0663,  0.1723,  0.2026, -0.1826, -0.1989,  0.1224, -0.1673, -0.1860,\n",
       "                       -0.0115, -0.0935, -0.1932,  0.3291,  0.2283,  0.1557, -0.1685,  0.2546]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.key.bias',\n",
       "              tensor([0.0029, 0.0107], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.values.weight',\n",
       "              tensor([[ 0.2125, -0.1061,  0.1323,  0.0908,  0.0040,  0.1589,  0.0711, -0.0916,\n",
       "                       -0.1470,  0.2051, -0.2004,  0.0005, -0.0606,  0.0802,  0.1483, -0.0177],\n",
       "                      [-0.0280, -0.0661,  0.1722, -0.0007, -0.0059,  0.0583, -0.1528, -0.0430,\n",
       "                       -0.0606, -0.1105,  0.0820,  0.1810, -0.0448, -0.0872,  0.0213,  0.2913]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.6.values.bias',\n",
       "              tensor([-0.1023,  0.2129], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.query.weight',\n",
       "              tensor([[-0.1794, -0.3999, -0.0521,  0.1264,  0.2985, -0.2887,  0.0058,  0.1932,\n",
       "                       -0.2170,  0.0127,  0.0064, -0.1755, -0.1612,  0.3630,  0.0436,  0.1090],\n",
       "                      [-0.2746, -0.3095, -0.3422, -0.0237, -0.0451, -0.1276, -0.0983,  0.4185,\n",
       "                        0.0428,  0.0587,  0.3731, -0.1224, -0.2997,  0.1734, -0.0761,  0.2608]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.query.bias',\n",
       "              tensor([ 0.1648, -0.0536], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.key.weight',\n",
       "              tensor([[ 5.6931e-03,  2.5693e-01,  2.0166e-01,  1.1331e-01, -5.0289e-02,\n",
       "                        1.0259e-01, -4.6944e-02, -2.8758e-01, -2.3720e-01, -1.5674e-02,\n",
       "                       -8.5816e-02,  1.0773e-02, -3.5539e-01, -4.4037e-01, -9.6648e-02,\n",
       "                       -3.9249e-02],\n",
       "                      [ 3.6912e-02,  3.1068e-01,  2.8671e-01,  3.0354e-01, -3.1656e-01,\n",
       "                       -3.4843e-02,  1.8788e-01, -2.2981e-01, -1.5485e-01, -1.8361e-01,\n",
       "                       -1.7768e-04,  1.7926e-02, -2.6030e-01, -2.0500e-01, -1.7014e-01,\n",
       "                       -2.0071e-01]], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.key.bias',\n",
       "              tensor([-0.2119,  0.1032], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.values.weight',\n",
       "              tensor([[-0.0219,  0.1779, -0.0270,  0.1565,  0.2124,  0.0830, -0.2108,  0.1621,\n",
       "                       -0.0499,  0.0773,  0.0308, -0.0435, -0.0915, -0.1189, -0.2011, -0.0426],\n",
       "                      [ 0.1015,  0.0927,  0.0572,  0.3051, -0.0503, -0.0617,  0.0280,  0.1234,\n",
       "                       -0.3159,  0.2030,  0.0489,  0.0733, -0.0095, -0.2433, -0.2993, -0.0104]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.heads.7.values.bias',\n",
       "              tensor([0.1422, 0.2582], device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.proj.weight',\n",
       "              tensor([[ 0.1719, -0.2415, -0.0321,  0.2197,  0.1007, -0.2769, -0.1348,  0.0133,\n",
       "                        0.0733,  0.2153, -0.0121, -0.0917, -0.0439,  0.0061,  0.0030, -0.2692],\n",
       "                      [-0.0970,  0.2167, -0.2023,  0.1067, -0.1665,  0.3494,  0.0078, -0.2927,\n",
       "                        0.1338, -0.0319, -0.2205,  0.0963,  0.1104, -0.0163, -0.0322,  0.3124],\n",
       "                      [ 0.0864,  0.0559,  0.1549, -0.0848, -0.0935, -0.2859,  0.0388,  0.0669,\n",
       "                       -0.0830, -0.2435, -0.1924, -0.1993, -0.1589,  0.0737,  0.0898,  0.3710],\n",
       "                      [ 0.0861,  0.2345,  0.2379, -0.1286,  0.1845, -0.0615, -0.1289,  0.0184,\n",
       "                       -0.0335, -0.1269,  0.1454,  0.0712,  0.1089, -0.0707,  0.0075, -0.1369],\n",
       "                      [-0.0208,  0.0870, -0.2029, -0.2008, -0.2284, -0.0425,  0.1386,  0.1113,\n",
       "                       -0.1068, -0.2558,  0.0502,  0.2476, -0.1381, -0.2265,  0.0492,  0.0427],\n",
       "                      [ 0.0723,  0.0252,  0.2656, -0.1841,  0.1799,  0.0521, -0.1451,  0.1445,\n",
       "                       -0.0121,  0.3232,  0.1894, -0.2416, -0.2934,  0.0260,  0.2409,  0.2478],\n",
       "                      [ 0.2203, -0.1314, -0.0296, -0.1333, -0.0289, -0.2072, -0.1185, -0.1020,\n",
       "                       -0.0302, -0.2549,  0.0212, -0.0945,  0.0406, -0.2119, -0.1902, -0.1635],\n",
       "                      [-0.1772,  0.1242, -0.1439, -0.1177,  0.0045,  0.0939,  0.1720,  0.1802,\n",
       "                        0.1747, -0.0799,  0.2515,  0.2044,  0.2009,  0.0434, -0.2405, -0.0271],\n",
       "                      [ 0.0274,  0.1308, -0.0566, -0.0072,  0.2164, -0.3458,  0.2402,  0.1224,\n",
       "                       -0.0600,  0.1284, -0.2387, -0.1057, -0.0777, -0.0256, -0.2084, -0.2446],\n",
       "                      [-0.0308,  0.1023, -0.2384,  0.1392,  0.1813,  0.1832, -0.1689, -0.3590,\n",
       "                       -0.2387,  0.0839, -0.0634, -0.2166,  0.1991, -0.1938, -0.0404,  0.1144],\n",
       "                      [-0.2291, -0.0059, -0.0123,  0.2045,  0.1181, -0.0500, -0.1993,  0.2956,\n",
       "                        0.2068, -0.0614, -0.2081, -0.2071,  0.1694, -0.0145, -0.2987, -0.3398],\n",
       "                      [ 0.0424, -0.0229,  0.0786, -0.0998, -0.2349,  0.0672, -0.0397, -0.2411,\n",
       "                        0.1207,  0.0434, -0.1375, -0.1484, -0.1688,  0.1904, -0.0045,  0.2417],\n",
       "                      [ 0.2109, -0.2120,  0.0559,  0.0152, -0.1833, -0.0589,  0.0806, -0.1880,\n",
       "                       -0.1764, -0.1046, -0.2503, -0.2371, -0.2222,  0.0757, -0.0210, -0.1163],\n",
       "                      [ 0.1836, -0.2209, -0.2380,  0.1819,  0.0095, -0.3116,  0.2029, -0.0308,\n",
       "                        0.0615, -0.2618,  0.0064, -0.2261, -0.0110,  0.0256,  0.1000,  0.1735],\n",
       "                      [-0.1735,  0.0287, -0.1476,  0.1585, -0.2360,  0.2898, -0.1010, -0.2559,\n",
       "                       -0.1092, -0.1726,  0.0244,  0.1369,  0.0188, -0.2774,  0.1470, -0.0733],\n",
       "                      [-0.0676, -0.1619,  0.0814, -0.0629, -0.1495,  0.0345, -0.1423, -0.1571,\n",
       "                        0.0396,  0.1205,  0.0714,  0.0041, -0.0044, -0.0007, -0.0850, -0.1304]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.self_attention.proj.bias',\n",
       "              tensor([ 0.1505,  0.1762,  0.0731, -0.1611, -0.0613, -0.0679,  0.0249, -0.1640,\n",
       "                      -0.2232,  0.1740,  0.1756, -0.1048,  0.1544,  0.0252, -0.2271, -0.2384],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.layer_norm2.weight',\n",
       "              tensor([1.0301, 1.0127, 0.9591, 1.0015, 0.9853, 0.9618, 0.9339, 1.0695, 1.0372,\n",
       "                      1.0011, 0.9846, 0.9900, 1.0163, 1.0280, 1.0351, 1.0434],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.layer_norm2.bias',\n",
       "              tensor([-0.0154,  0.0071, -0.0110,  0.0071,  0.0008,  0.0112, -0.0118, -0.0047,\n",
       "                      -0.0046, -0.0030, -0.0037, -0.0039, -0.0029,  0.0105,  0.0041, -0.0017],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.mlp.mlp.0.weight',\n",
       "              tensor([[-0.1839,  0.0586,  0.0161,  ...,  0.0020, -0.1110,  0.1384],\n",
       "                      [ 0.1838, -0.1383,  0.1711,  ..., -0.1562, -0.2451,  0.0695],\n",
       "                      [ 0.0152, -0.1796,  0.1870,  ...,  0.0730,  0.0469, -0.2146],\n",
       "                      ...,\n",
       "                      [-0.1224,  0.1262, -0.0890,  ..., -0.0347,  0.2123,  0.0847],\n",
       "                      [-0.1403,  0.2307, -0.2558,  ..., -0.1373, -0.1081,  0.0691],\n",
       "                      [-0.1096, -0.2423, -0.1280,  ...,  0.2034,  0.0125,  0.0742]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.mlp.mlp.0.bias',\n",
       "              tensor([ 0.0516,  0.0853,  0.2170,  0.0705, -0.0011, -0.0399,  0.0307, -0.2045,\n",
       "                       0.1881, -0.0652, -0.2368, -0.1620, -0.0280,  0.0265, -0.1438,  0.1100,\n",
       "                      -0.2272, -0.1900,  0.2477,  0.0031, -0.1972,  0.2526, -0.1035, -0.1595,\n",
       "                       0.2467, -0.0423,  0.0937, -0.1773,  0.0839, -0.1312, -0.1767,  0.1595,\n",
       "                      -0.1975, -0.0091,  0.1397, -0.1485,  0.0573,  0.1539, -0.0820,  0.2186,\n",
       "                       0.0922, -0.1224, -0.0417,  0.0174,  0.0271,  0.1776,  0.0849, -0.1787,\n",
       "                       0.1285, -0.0829,  0.0487, -0.0154,  0.1260,  0.0828,  0.0537,  0.1659,\n",
       "                      -0.1234,  0.2686, -0.2508,  0.0812, -0.0164, -0.2001,  0.1987,  0.2427],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.mlp.mlp.2.weight',\n",
       "              tensor([[-0.0424,  0.1001,  0.0580,  ...,  0.1447,  0.0923, -0.0279],\n",
       "                      [ 0.0461,  0.1647,  0.0415,  ...,  0.0991,  0.0108, -0.0672],\n",
       "                      [-0.1448,  0.1052, -0.0969,  ...,  0.0798, -0.0342, -0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0560,  0.0431, -0.0112,  ..., -0.1331, -0.0444, -0.0986],\n",
       "                      [ 0.1250,  0.0150,  0.1126,  ...,  0.0478, -0.0145, -0.0938],\n",
       "                      [-0.0659, -0.1121, -0.0972,  ...,  0.0059,  0.0942,  0.0353]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.1.mlp.mlp.2.bias',\n",
       "              tensor([ 0.0304,  0.0531,  0.0371, -0.0193,  0.0153, -0.1105, -0.1006, -0.0713,\n",
       "                      -0.0742,  0.0154,  0.1001,  0.0076,  0.0041, -0.0635,  0.0580, -0.0712],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.layer_norm1.weight',\n",
       "              tensor([1.0224, 1.0275, 0.9950, 1.0208, 0.9832, 0.9905, 0.9753, 1.1812, 1.0211,\n",
       "                      1.0303, 0.9984, 0.9910, 1.0208, 1.0630, 1.0189, 1.0238],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.layer_norm1.bias',\n",
       "              tensor([-0.0188, -0.0053, -0.0226,  0.0027,  0.0034, -0.0363,  0.0057, -0.0019,\n",
       "                      -0.0008,  0.0042,  0.0176, -0.0099, -0.0053,  0.0288,  0.0110, -0.0093],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.query.weight',\n",
       "              tensor([[-0.1519,  0.0075,  0.2673,  0.0278, -0.2181,  0.0723, -0.1691,  0.4006,\n",
       "                        0.0467, -0.2287, -0.0497,  0.1242,  0.2478, -0.3191,  0.0183,  0.2811],\n",
       "                      [ 0.1629,  0.0842,  0.1234, -0.0048, -0.0776, -0.1152,  0.3196, -0.4658,\n",
       "                       -0.2333,  0.2739,  0.1173, -0.2798, -0.0287,  0.4092, -0.1760, -0.0700]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.query.bias',\n",
       "              tensor([-0.3738, -0.0730], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.key.weight',\n",
       "              tensor([[ 0.0276, -0.4077, -0.1886, -0.2244,  0.1447, -0.2765, -0.0214, -0.2716,\n",
       "                       -0.1707,  0.1556,  0.3744, -0.1267,  0.2449,  0.0303, -0.0344,  0.2669],\n",
       "                      [-0.0376,  0.1107,  0.1598, -0.1186, -0.3263,  0.2908, -0.3177, -0.1614,\n",
       "                        0.0693, -0.0642, -0.1514,  0.0831, -0.1234, -0.3616,  0.1838, -0.3768]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.key.bias',\n",
       "              tensor([0.1714, 0.0022], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.values.weight',\n",
       "              tensor([[ 0.2043, -0.0704, -0.0360,  0.1721, -0.1232,  0.2231, -0.0377, -0.0735,\n",
       "                       -0.2146,  0.1015, -0.0466, -0.3012,  0.1556,  0.1291, -0.1078,  0.0825],\n",
       "                      [ 0.1914,  0.2935, -0.1860, -0.1033,  0.2299,  0.0112,  0.0774, -0.3735,\n",
       "                       -0.1532,  0.3018, -0.0820,  0.0287, -0.2347, -0.1212, -0.0406,  0.2045]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.0.values.bias',\n",
       "              tensor([-0.0058, -0.1090], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.2327,  0.3111,  0.2983, -0.2529, -0.2971,  0.2276, -0.3718,  0.0797,\n",
       "                        0.2005, -0.2154, -0.4159,  0.3604,  0.1469, -0.2634, -0.0149,  0.3217],\n",
       "                      [-0.1815, -0.0416, -0.1530, -0.0776, -0.0678, -0.0277, -0.1261,  0.2025,\n",
       "                        0.0300, -0.3169, -0.1201, -0.2696,  0.2344, -0.1788, -0.1201,  0.0911]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.query.bias',\n",
       "              tensor([-0.1780,  0.1162], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.key.weight',\n",
       "              tensor([[-0.0060,  0.3471,  0.1515,  0.3495, -0.3729,  0.0606, -0.0720, -0.2494,\n",
       "                       -0.3333,  0.0033, -0.0328, -0.3447, -0.2664, -0.2399, -0.1190, -0.0645],\n",
       "                      [ 0.0125,  0.1615, -0.0385,  0.2932,  0.0258,  0.1789, -0.2766,  0.1502,\n",
       "                       -0.0049, -0.2505, -0.1561, -0.0812,  0.0563, -0.4085, -0.0730,  0.1231]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.key.bias',\n",
       "              tensor([0.2173, 0.0047], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.values.weight',\n",
       "              tensor([[-0.0790, -0.2094, -0.0066, -0.0573, -0.0568,  0.0370,  0.0683,  0.2722,\n",
       "                        0.1916,  0.2322,  0.0785, -0.1434, -0.0363,  0.0721, -0.0800, -0.0019],\n",
       "                      [ 0.2330,  0.0332,  0.1216, -0.1310, -0.2266, -0.0117, -0.0716, -0.2996,\n",
       "                       -0.2361, -0.1609,  0.2285, -0.0083, -0.0398,  0.1286, -0.1064,  0.0146]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.1.values.bias',\n",
       "              tensor([0.2059, 0.0013], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.query.weight',\n",
       "              tensor([[ 0.2038,  0.0810, -0.1235, -0.0007, -0.2190,  0.1543, -0.0627, -0.2139,\n",
       "                        0.0722, -0.1829,  0.0996, -0.2084,  0.0610,  0.0166, -0.0741, -0.2890],\n",
       "                      [ 0.0452,  0.4321,  0.2207,  0.1855, -0.2730,  0.2201, -0.2372, -0.1487,\n",
       "                       -0.0843, -0.2176, -0.0399,  0.0745, -0.0247, -0.0257, -0.1619, -0.0261]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.query.bias',\n",
       "              tensor([-0.0885, -0.1507], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.key.weight',\n",
       "              tensor([[-0.1280,  0.1142, -0.0685,  0.0944, -0.1236, -0.1921,  0.0065,  0.3066,\n",
       "                       -0.0195, -0.1165,  0.2115, -0.0505, -0.1735, -0.0180,  0.2766,  0.0649],\n",
       "                      [-0.2126, -0.1616,  0.0554, -0.2445, -0.0354,  0.0276, -0.2657,  0.2507,\n",
       "                        0.2995, -0.4963, -0.0667, -0.1793, -0.2063, -0.0344,  0.2217,  0.1832]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.key.bias',\n",
       "              tensor([-0.1523, -0.0086], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.values.weight',\n",
       "              tensor([[-0.1508, -0.0093, -0.0642,  0.2667,  0.1444, -0.2120,  0.2138,  0.3586,\n",
       "                       -0.0878,  0.1093, -0.2304, -0.0085,  0.0457, -0.0173,  0.2551, -0.2927],\n",
       "                      [ 0.1059,  0.0574, -0.0012, -0.2583,  0.2166, -0.1734, -0.2210, -0.1456,\n",
       "                       -0.1710, -0.2269,  0.2151,  0.1252, -0.1660,  0.1472, -0.1848,  0.2613]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.2.values.bias',\n",
       "              tensor([-0.0823,  0.0247], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.query.weight',\n",
       "              tensor([[-0.1866,  0.1573,  0.0789,  0.0021,  0.2650, -0.3047,  0.1580, -0.2087,\n",
       "                       -0.4220,  0.3412, -0.1292, -0.2282,  0.0325,  0.1503,  0.1026, -0.2343],\n",
       "                      [ 0.1288,  0.2875, -0.2898,  0.0052,  0.2776, -0.1261,  0.0776, -0.3255,\n",
       "                       -0.3421,  0.0860,  0.3019, -0.1613, -0.3786,  0.0032, -0.0759, -0.1789]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.query.bias',\n",
       "              tensor([ 0.2860, -0.0652], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.key.weight',\n",
       "              tensor([[ 0.2315,  0.2715,  0.2116, -0.0359, -0.1831,  0.3089,  0.3354,  0.0166,\n",
       "                       -0.3339, -0.0012, -0.1129, -0.2736, -0.3564, -0.2722, -0.0315, -0.2396],\n",
       "                      [-0.0281,  0.1740, -0.1553,  0.0964,  0.0904,  0.2080, -0.0079, -0.1047,\n",
       "                       -0.2638,  0.3109, -0.2979, -0.1478, -0.2301, -0.1762, -0.2978,  0.0535]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.key.bias',\n",
       "              tensor([0.1613, 0.1159], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.values.weight',\n",
       "              tensor([[-0.1154,  0.1205, -0.1463, -0.1145,  0.0992, -0.2574,  0.0672,  0.1025,\n",
       "                       -0.0759, -0.0922, -0.0582,  0.0956,  0.1235, -0.2195, -0.0106,  0.0969],\n",
       "                      [ 0.3155,  0.2039,  0.2245,  0.3410,  0.0445,  0.0982, -0.2065, -0.0514,\n",
       "                       -0.2924,  0.2988, -0.1276, -0.0734, -0.2681,  0.1783,  0.1722, -0.2110]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.3.values.bias',\n",
       "              tensor([0.0828, 0.1308], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 0.0076,  0.1170, -0.2053, -0.2659,  0.3466,  0.0658, -0.2067, -0.3514,\n",
       "                       -0.0599,  0.4762,  0.3662, -0.1651, -0.0943,  0.1493,  0.2590,  0.1634],\n",
       "                      [-0.3669, -0.1686, -0.5604,  0.0170,  0.4081, -0.2499,  0.2469, -0.3250,\n",
       "                       -0.5528,  0.3981,  0.3801,  0.0935,  0.1854,  0.1122,  0.3437, -0.1211]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.query.bias',\n",
       "              tensor([0.0314, 0.1814], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.key.weight',\n",
       "              tensor([[ 0.1730,  0.3528,  0.5078, -0.1103, -0.4750,  0.3103, -0.2616, -0.1533,\n",
       "                       -0.3458, -0.0230, -0.0534, -0.3105, -0.0052,  0.1049, -0.2560,  0.2170],\n",
       "                      [ 0.1232,  0.5145,  0.5356,  0.1701, -0.1119,  0.4244,  0.0064,  0.0655,\n",
       "                       -0.1496, -0.0342, -0.0166, -0.1097,  0.0045,  0.0090, -0.3267, -0.0192]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.key.bias',\n",
       "              tensor([0.0688, 0.2524], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.values.weight',\n",
       "              tensor([[-0.0828,  0.2316,  0.0461,  0.1537, -0.2330, -0.1003,  0.1888,  0.0878,\n",
       "                        0.0828, -0.0566,  0.2204, -0.2077, -0.2216,  0.0131, -0.1101, -0.2375],\n",
       "                      [ 0.1281,  0.0389, -0.0806, -0.0130, -0.2014,  0.0892,  0.1099,  0.4900,\n",
       "                       -0.1147, -0.1570, -0.2232, -0.2027, -0.2865, -0.1563,  0.2082, -0.4209]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.4.values.bias',\n",
       "              tensor([-0.2165,  0.1811], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.0817, -0.1698, -0.2637, -0.2085,  0.2255, -0.0293, -0.1189,  0.0074,\n",
       "                       -0.0433, -0.1134,  0.1737, -0.0823,  0.3048,  0.3222, -0.0407, -0.2213],\n",
       "                      [-0.2383, -0.2157, -0.1784, -0.2564, -0.2498, -0.2014, -0.2337, -0.1022,\n",
       "                        0.0412,  0.0919, -0.1028,  0.1649, -0.2109, -0.2534, -0.0340,  0.1802]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.query.bias',\n",
       "              tensor([0.1988, 0.0837], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.key.weight',\n",
       "              tensor([[ 0.3411,  0.2305,  0.0351,  0.3077, -0.0944,  0.0971,  0.1507, -0.2848,\n",
       "                        0.2171,  0.1177,  0.2075,  0.0216, -0.2947, -0.3364,  0.1513,  0.1558],\n",
       "                      [ 0.0384, -0.0021, -0.2174, -0.0966, -0.1842,  0.0905, -0.2227,  0.1222,\n",
       "                        0.0812,  0.1029, -0.2280,  0.1291,  0.1192, -0.0632, -0.2001, -0.0010]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.key.bias',\n",
       "              tensor([-0.0903,  0.1166], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.values.weight',\n",
       "              tensor([[ 0.2740,  0.0404,  0.1536,  0.0477, -0.1007,  0.1308, -0.1826, -0.2563,\n",
       "                        0.1903,  0.0776, -0.0621,  0.1914,  0.1723,  0.1780, -0.1887,  0.0870],\n",
       "                      [ 0.0852, -0.0415, -0.1113,  0.2270, -0.0883,  0.0467, -0.0851,  0.1899,\n",
       "                        0.0519, -0.0427,  0.1636, -0.2857,  0.2319,  0.1249, -0.0367,  0.0752]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.5.values.bias',\n",
       "              tensor([-0.0292, -0.1815], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.query.weight',\n",
       "              tensor([[-0.3400,  0.1581,  0.1333, -0.0349,  0.1974, -0.0272,  0.2885,  0.0739,\n",
       "                       -0.1116,  0.1248,  0.2083, -0.1318,  0.0300,  0.3399,  0.2306, -0.0104],\n",
       "                      [ 0.0382, -0.2901, -0.1698,  0.2133, -0.2469,  0.1262,  0.2761,  0.0343,\n",
       "                       -0.1155, -0.2099, -0.1652, -0.1141, -0.3287, -0.0010,  0.3153, -0.3020]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.query.bias',\n",
       "              tensor([ 0.1998, -0.0130], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.key.weight',\n",
       "              tensor([[ 2.2225e-01,  2.7347e-01,  5.5072e-02, -5.7531e-02, -9.4207e-02,\n",
       "                       -1.9674e-01,  2.0340e-01, -1.6423e-01,  1.1784e-01,  1.2495e-01,\n",
       "                        9.7969e-02,  1.5211e-01, -1.8447e-01,  1.6319e-01, -2.2575e-01,\n",
       "                       -3.4305e-04],\n",
       "                      [ 5.0663e-02, -1.2151e-01,  1.5674e-01, -1.2120e-01,  4.6758e-01,\n",
       "                       -1.8786e-01,  9.0980e-02,  9.5118e-02, -7.4893e-02,  2.2261e-01,\n",
       "                        1.6606e-01, -3.7769e-02,  3.3010e-01,  3.2216e-01,  2.4122e-01,\n",
       "                        8.6856e-02]], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.key.bias',\n",
       "              tensor([ 0.1057, -0.2031], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.values.weight',\n",
       "              tensor([[ 0.1214, -0.0809,  0.2040, -0.0600, -0.0429,  0.2436,  0.2443,  0.1616,\n",
       "                       -0.0571, -0.1970, -0.0722,  0.0949,  0.1839,  0.2007, -0.0275, -0.2181],\n",
       "                      [-0.1702,  0.1908,  0.0667,  0.0624, -0.1017, -0.2088,  0.0985, -0.3478,\n",
       "                        0.0922, -0.2037,  0.1116,  0.0966, -0.0829,  0.0436, -0.3286, -0.2042]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.6.values.bias',\n",
       "              tensor([-0.2263,  0.1538], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.query.weight',\n",
       "              tensor([[-0.3397, -0.1523, -0.1443, -0.2877,  0.1337,  0.0994,  0.1040,  0.2625,\n",
       "                       -0.0726,  0.2146,  0.2742, -0.0743,  0.3886, -0.1249,  0.2514,  0.1092],\n",
       "                      [ 0.3123, -0.0644, -0.1080,  0.0622,  0.0078,  0.2052,  0.1451, -0.3551,\n",
       "                       -0.3035,  0.2523, -0.1554, -0.0531,  0.0597,  0.4010, -0.1414,  0.1467]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.query.bias',\n",
       "              tensor([-0.0462,  0.1866], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.key.weight',\n",
       "              tensor([[-0.1872, -0.0487, -0.0254, -0.0469,  0.0032,  0.1933, -0.0292,  0.1301,\n",
       "                        0.3802, -0.3398, -0.0470,  0.2238,  0.3930, -0.0270, -0.0883,  0.1287],\n",
       "                      [ 0.2393,  0.2892, -0.2368,  0.3954, -0.2153,  0.1764,  0.0679,  0.0475,\n",
       "                       -0.1186,  0.1888,  0.0239, -0.1979,  0.0131, -0.0113,  0.0951, -0.1978]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.key.bias',\n",
       "              tensor([-0.2143, -0.1348], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.values.weight',\n",
       "              tensor([[-0.0060,  0.1369,  0.0445,  0.1279, -0.1279, -0.1096, -0.0513,  0.3025,\n",
       "                       -0.0221, -0.2160,  0.0866, -0.2568,  0.2247,  0.1033,  0.2199,  0.0199],\n",
       "                      [-0.1827,  0.2062,  0.2463,  0.0182,  0.0631, -0.2025,  0.2279, -0.4404,\n",
       "                        0.1581,  0.0513, -0.0144,  0.0432, -0.1702,  0.0124, -0.0800,  0.2418]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.heads.7.values.bias',\n",
       "              tensor([ 0.2238, -0.1817], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.proj.weight',\n",
       "              tensor([[-2.6677e-01, -3.2385e-01, -2.4918e-01,  8.1055e-02,  2.3680e-01,\n",
       "                        7.3220e-02,  1.5138e-01,  4.5698e-02, -9.2964e-02,  3.1345e-02,\n",
       "                       -1.8906e-01, -1.6412e-01, -1.4509e-01,  1.0401e-01,  1.1481e-01,\n",
       "                       -2.4882e-01],\n",
       "                      [-9.7158e-02, -6.5362e-02, -8.3875e-02,  1.0965e-01, -1.9058e-01,\n",
       "                       -4.8554e-02, -1.8539e-01, -1.3144e-01,  8.7631e-02, -2.4718e-01,\n",
       "                       -1.0783e-01,  8.5010e-02,  1.3310e-01,  1.2994e-01, -1.8855e-01,\n",
       "                       -1.7268e-01],\n",
       "                      [-1.1809e-01, -5.7827e-02,  1.9139e-01, -1.8675e-02, -1.3635e-01,\n",
       "                       -1.0537e-01,  1.6470e-01,  1.3987e-01,  2.2834e-01, -9.9339e-02,\n",
       "                       -9.6802e-02, -9.2366e-02,  2.4556e-01, -1.3093e-01,  1.6033e-01,\n",
       "                        3.2224e-02],\n",
       "                      [ 1.1719e-01, -2.1633e-01,  9.2388e-02, -2.6660e-04,  4.4652e-02,\n",
       "                       -6.5031e-02,  5.9635e-02, -2.8339e-01, -2.8805e-01, -1.5120e-02,\n",
       "                        2.2346e-01,  1.3508e-01,  2.5183e-01, -9.1886e-02, -4.0797e-02,\n",
       "                        9.5774e-02],\n",
       "                      [ 1.6822e-01,  3.3343e-01, -1.4839e-01,  2.3479e-02,  8.8511e-02,\n",
       "                       -4.4683e-02, -2.0080e-01, -1.7783e-02, -1.9005e-01, -5.3412e-02,\n",
       "                        2.4628e-01, -6.5122e-02, -1.1465e-01,  2.3999e-01, -9.4532e-02,\n",
       "                        3.3427e-01],\n",
       "                      [-2.4446e-01,  8.1842e-02, -1.9849e-01,  2.6822e-01, -1.3315e-01,\n",
       "                       -1.3302e-01, -1.6355e-01, -2.1949e-01, -1.1466e-01, -2.1033e-01,\n",
       "                       -6.3544e-02, -7.4987e-02, -1.7605e-01,  1.8663e-01,  7.8755e-02,\n",
       "                        2.0536e-01],\n",
       "                      [ 1.4290e-01, -3.2450e-01,  2.8262e-01, -2.0460e-01,  1.4649e-01,\n",
       "                        3.5163e-03, -2.8679e-01,  5.8945e-02, -6.7687e-02,  2.1388e-01,\n",
       "                        4.7679e-02,  2.2695e-01,  1.3302e-01, -2.3744e-02,  8.9531e-02,\n",
       "                       -3.2680e-01],\n",
       "                      [-4.6467e-02, -1.8460e-01,  2.8799e-02,  2.6726e-02,  1.1077e-01,\n",
       "                        1.3994e-01, -2.4831e-01,  8.2177e-02, -2.4730e-01,  9.6603e-02,\n",
       "                       -1.8587e-01, -2.3446e-01, -1.5636e-01, -7.7836e-02, -2.1598e-01,\n",
       "                        1.3164e-01],\n",
       "                      [-9.0913e-02,  6.7858e-02,  5.8581e-03,  2.4265e-01,  6.6790e-02,\n",
       "                        1.8961e-01,  7.8636e-02, -2.1368e-01, -3.0122e-01, -2.1275e-01,\n",
       "                        1.8625e-01,  8.9888e-02, -2.1815e-01,  7.1566e-02,  2.4435e-02,\n",
       "                        1.0635e-01],\n",
       "                      [ 1.8689e-01,  1.6869e-01,  1.3383e-01, -1.1721e-01,  8.8764e-02,\n",
       "                       -1.4300e-01, -2.2199e-02, -6.3447e-02,  8.7687e-02,  8.1590e-02,\n",
       "                        2.8776e-02, -7.1947e-02, -4.6584e-02, -4.6791e-03,  2.5116e-01,\n",
       "                       -1.3343e-01],\n",
       "                      [-3.4609e-02, -2.7141e-01,  1.6686e-01,  1.4589e-01, -1.9077e-01,\n",
       "                        2.5697e-01, -6.0519e-02, -1.6280e-02, -3.2930e-03, -1.0852e-01,\n",
       "                       -1.8010e-01,  1.2336e-02,  1.3086e-01, -4.6230e-02,  3.0131e-01,\n",
       "                       -1.6400e-01],\n",
       "                      [ 8.0188e-02,  2.8706e-01, -2.6253e-01, -1.8102e-01, -2.1176e-01,\n",
       "                        1.0249e-01, -2.4360e-01,  1.9503e-01, -1.2185e-01, -2.4073e-01,\n",
       "                        1.0552e-02, -2.0687e-02, -6.7632e-03, -2.4075e-01, -1.0827e-01,\n",
       "                        2.1116e-01],\n",
       "                      [-1.7378e-02, -9.8213e-03,  2.9022e-02, -1.1880e-01,  2.1293e-01,\n",
       "                       -3.8806e-02, -2.4821e-01,  3.3173e-02,  2.3445e-01,  1.1450e-01,\n",
       "                       -7.5817e-02, -9.4525e-02,  1.2843e-01,  1.1328e-01, -7.5241e-03,\n",
       "                       -1.9533e-01],\n",
       "                      [-1.2633e-01, -1.7190e-01, -8.2181e-02, -1.3822e-01, -9.4818e-02,\n",
       "                        5.9719e-02, -1.7670e-01,  2.5651e-01,  1.4973e-01, -5.8967e-02,\n",
       "                       -2.0699e-01,  1.4782e-01, -1.3162e-01,  7.7579e-02,  1.9261e-01,\n",
       "                        7.8920e-02],\n",
       "                      [-1.7833e-01,  2.8682e-02,  5.3223e-02, -9.8367e-02,  5.8955e-02,\n",
       "                        7.1096e-02,  1.8101e-01,  1.9997e-01,  2.6314e-01,  2.8573e-03,\n",
       "                       -1.5760e-01, -1.0060e-01,  5.3446e-02,  1.1529e-01, -2.4451e-01,\n",
       "                        1.2321e-01],\n",
       "                      [-3.5666e-02,  1.6900e-01,  1.9340e-01,  7.2333e-02,  1.9158e-01,\n",
       "                        5.9734e-02, -2.4577e-01,  2.2157e-01,  1.6320e-02, -9.4106e-02,\n",
       "                        1.4177e-01, -1.1731e-02, -2.4549e-01, -5.3629e-02, -1.9502e-01,\n",
       "                       -5.6820e-02]], device='cuda:0')),\n",
       "             ('tab_layers.2.self_attention.proj.bias',\n",
       "              tensor([-0.0573, -0.2026, -0.1938, -0.0041, -0.1115, -0.1698, -0.1001, -0.0048,\n",
       "                       0.0957,  0.1751, -0.0394, -0.0380, -0.1899, -0.0456, -0.1207,  0.0182],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.layer_norm2.weight',\n",
       "              tensor([1.0471, 0.9932, 0.9794, 1.0314, 0.9700, 0.9983, 0.9793, 0.9468, 1.0496,\n",
       "                      1.0115, 1.0096, 1.0058, 0.9492, 1.0442, 1.0173, 1.0317],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.layer_norm2.bias',\n",
       "              tensor([-0.0186, -0.0071, -0.0144,  0.0034,  0.0036, -0.0143,  0.0004,  0.0105,\n",
       "                      -0.0053,  0.0082,  0.0196, -0.0075, -0.0037,  0.0257,  0.0094,  0.0031],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.mlp.mlp.0.weight',\n",
       "              tensor([[ 0.1489,  0.1226,  0.1358,  ..., -0.2004,  0.1133, -0.1847],\n",
       "                      [ 0.0437,  0.0791, -0.0856,  ...,  0.0476,  0.2520, -0.0171],\n",
       "                      [-0.1436,  0.1173,  0.0189,  ...,  0.1718,  0.1166, -0.0424],\n",
       "                      ...,\n",
       "                      [-0.0976, -0.1096,  0.0858,  ..., -0.0986,  0.1183,  0.0776],\n",
       "                      [-0.1828,  0.1051,  0.2231,  ..., -0.0646, -0.0057,  0.0445],\n",
       "                      [-0.2547,  0.0938,  0.0437,  ...,  0.0528,  0.0381, -0.0756]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.mlp.mlp.0.bias',\n",
       "              tensor([ 0.0963,  0.1156,  0.1516, -0.1940,  0.0963, -0.0956,  0.0991, -0.2332,\n",
       "                      -0.1141, -0.1304,  0.1461, -0.1981,  0.1953,  0.0153, -0.2139, -0.0530,\n",
       "                       0.1906,  0.0243,  0.0517,  0.0451, -0.0872, -0.1988,  0.0752, -0.0496,\n",
       "                      -0.1169,  0.0725, -0.0983, -0.1389,  0.1385,  0.1623,  0.1237,  0.1905,\n",
       "                      -0.2037, -0.0309, -0.1432,  0.1906, -0.0608, -0.1812,  0.0249,  0.2213,\n",
       "                      -0.1873, -0.0284,  0.1023, -0.0161,  0.0103,  0.0455, -0.0732,  0.1010,\n",
       "                      -0.0070, -0.0390,  0.1145,  0.0756, -0.1568,  0.0933,  0.1143, -0.1533,\n",
       "                       0.2360,  0.2015,  0.0135,  0.0436, -0.0552, -0.2099,  0.1973, -0.0541],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.mlp.mlp.2.weight',\n",
       "              tensor([[ 0.0198, -0.0499, -0.0226,  ...,  0.0322, -0.0486, -0.1078],\n",
       "                      [ 0.0262, -0.0746,  0.1102,  ..., -0.0421, -0.1133, -0.0768],\n",
       "                      [ 0.0344,  0.0343,  0.0027,  ...,  0.0516, -0.0633, -0.1167],\n",
       "                      ...,\n",
       "                      [-0.1447, -0.0253, -0.1432,  ...,  0.0824,  0.0986,  0.0334],\n",
       "                      [-0.1188,  0.0767, -0.0428,  ..., -0.0332,  0.1131,  0.0736],\n",
       "                      [-0.0530, -0.0260, -0.0311,  ..., -0.0699, -0.0262,  0.0418]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.2.mlp.mlp.2.bias',\n",
       "              tensor([ 0.0125, -0.0981, -0.1039,  0.0627, -0.0634, -0.0108,  0.0244, -0.0237,\n",
       "                      -0.0641,  0.1159,  0.0652,  0.0127, -0.0246,  0.0892, -0.0826, -0.0183],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.layer_norm1.weight',\n",
       "              tensor([1.0314, 1.0078, 0.9959, 1.0466, 0.9945, 1.0139, 0.9759, 0.9836, 1.0752,\n",
       "                      1.0133, 1.0021, 1.0088, 1.0293, 1.0521, 1.0170, 1.0382],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.layer_norm1.bias',\n",
       "              tensor([-0.0139,  0.0011, -0.0260, -0.0145, -0.0015, -0.0139, -0.0057,  0.0060,\n",
       "                      -0.0030,  0.0114,  0.0143, -0.0084, -0.0083,  0.0123,  0.0178,  0.0001],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.query.weight',\n",
       "              tensor([[-0.0966, -0.2802, -0.3433, -0.3812,  0.0829,  0.1850, -0.0452,  0.1287,\n",
       "                        0.0930, -0.0291,  0.3129,  0.0052,  0.0420,  0.1991,  0.2286,  0.2185],\n",
       "                      [ 0.3674,  0.3859,  0.0552,  0.2657, -0.1315, -0.1553, -0.0118, -0.0016,\n",
       "                       -0.3899,  0.0287, -0.3021, -0.3416,  0.2143, -0.0532,  0.0158, -0.0574]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.query.bias',\n",
       "              tensor([ 0.4321, -0.0357], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.key.weight',\n",
       "              tensor([[ 0.0052, -0.0838,  0.1345,  0.4023, -0.2632,  0.2467, -0.1247,  0.2886,\n",
       "                       -0.0170, -0.0405, -0.1494, -0.4371, -0.1944, -0.3919, -0.0509, -0.3069],\n",
       "                      [-0.2783, -0.1731, -0.0960, -0.4214,  0.0220, -0.1871,  0.1751,  0.0910,\n",
       "                        0.0764,  0.2637,  0.1766,  0.3160,  0.3200,  0.0936,  0.0178,  0.3051]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.key.bias',\n",
       "              tensor([-0.0064, -0.0566], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.values.weight',\n",
       "              tensor([[-0.2338, -0.2822,  0.1570,  0.0654,  0.2887, -0.1613, -0.2259,  0.1528,\n",
       "                        0.3090,  0.1438,  0.2158, -0.0746,  0.1637,  0.2157, -0.1443,  0.1997],\n",
       "                      [-0.0388, -0.0079,  0.0119, -0.1330, -0.0177, -0.1022, -0.0773,  0.0185,\n",
       "                        0.1112, -0.1009,  0.1715, -0.1916, -0.1938,  0.2198, -0.0900,  0.0149]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.0.values.bias',\n",
       "              tensor([ 0.1827, -0.0763], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.1673, -0.0137,  0.1385, -0.0653, -0.0269,  0.2379,  0.1767, -0.0508,\n",
       "                       -0.1479, -0.0594,  0.0131,  0.2962,  0.1406, -0.0770, -0.2184,  0.0683],\n",
       "                      [-0.0762, -0.2096, -0.2034, -0.2091,  0.0415, -0.1931,  0.0018, -0.0294,\n",
       "                       -0.2871,  0.3204,  0.1055, -0.2323, -0.2223,  0.4496,  0.3109,  0.1500]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.query.bias',\n",
       "              tensor([-0.3482, -0.0317], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.key.weight',\n",
       "              tensor([[-0.0460, -0.2077,  0.0741, -0.0454, -0.0424,  0.0611,  0.2193, -0.2377,\n",
       "                       -0.0056, -0.3301, -0.0349, -0.1431,  0.2448,  0.2964, -0.3883, -0.0257],\n",
       "                      [-0.0529, -0.0031, -0.2672,  0.2140,  0.2645, -0.1384, -0.0223, -0.0695,\n",
       "                        0.0399,  0.0079,  0.2894, -0.1046, -0.1069, -0.2589, -0.1668, -0.2798]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.key.bias',\n",
       "              tensor([-0.1682,  0.0599], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.values.weight',\n",
       "              tensor([[ 0.2568, -0.0714,  0.0562,  0.0489,  0.0367,  0.1708,  0.1197, -0.0069,\n",
       "                        0.1140, -0.1367,  0.2127,  0.1193,  0.1261,  0.1675, -0.0726, -0.0648],\n",
       "                      [ 0.0765, -0.1810, -0.0773,  0.2455,  0.0529,  0.1677, -0.2152,  0.1314,\n",
       "                        0.1903,  0.2049,  0.1332, -0.0586,  0.1186, -0.2375, -0.0953,  0.1799]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.1.values.bias',\n",
       "              tensor([ 0.0402, -0.0024], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.query.weight',\n",
       "              tensor([[ 0.0654,  0.1730,  0.2853,  0.1664,  0.2098,  0.3236, -0.0435,  0.1268,\n",
       "                        0.2151, -0.0396,  0.1208,  0.0454, -0.1716, -0.2288, -0.1212, -0.1556],\n",
       "                      [-0.0300, -0.0118,  0.1733,  0.0788,  0.2691, -0.1405, -0.1610,  0.0178,\n",
       "                       -0.1219,  0.1876,  0.0749, -0.2106,  0.1882, -0.0915, -0.0244,  0.0738]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.query.bias',\n",
       "              tensor([0.0533, 0.1859], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.key.weight',\n",
       "              tensor([[-0.0488, -0.0834, -0.1859, -0.0454, -0.1544, -0.2286,  0.3019, -0.0695,\n",
       "                       -0.2632,  0.2420,  0.2496, -0.2641, -0.2402,  0.2890, -0.0252, -0.0176],\n",
       "                      [-0.2455, -0.0787,  0.0208, -0.1846, -0.0092, -0.0047,  0.0328, -0.0980,\n",
       "                       -0.1174, -0.0268, -0.1979, -0.1232, -0.0742, -0.0671,  0.1582, -0.1387]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.key.bias',\n",
       "              tensor([ 0.1095, -0.0713], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.values.weight',\n",
       "              tensor([[-0.0975,  0.0315, -0.1124,  0.2227, -0.2491, -0.1321,  0.0113, -0.0357,\n",
       "                       -0.0810,  0.0227, -0.0553, -0.1206, -0.1147,  0.0114,  0.0046, -0.0462],\n",
       "                      [-0.0849, -0.0139, -0.1861, -0.0514, -0.1167, -0.2318, -0.1096,  0.0236,\n",
       "                        0.2510, -0.2495, -0.1546, -0.0742, -0.1182, -0.0772,  0.1821,  0.0895]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.2.values.bias',\n",
       "              tensor([ 0.1212, -0.0103], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.query.weight',\n",
       "              tensor([[-0.4130, -0.2474, -0.2090, -0.0438, -0.1373, -0.0600, -0.0706,  0.1034,\n",
       "                        0.0433,  0.3862, -0.0569, -0.1626, -0.3473,  0.3717, -0.1505,  0.1473],\n",
       "                      [ 0.4727,  0.0592,  0.3324,  0.4640, -0.4407,  0.3143,  0.2349,  0.4076,\n",
       "                       -0.2459, -0.0641, -0.4121,  0.2264,  0.2851,  0.0247, -0.0397, -0.3454]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.query.bias',\n",
       "              tensor([-0.0975, -0.0985], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.key.weight',\n",
       "              tensor([[ 0.1861, -0.0463,  0.0426,  0.1099, -0.1061,  0.4314,  0.1960,  0.1210,\n",
       "                        0.1478,  0.0112,  0.2199, -0.0109, -0.1090, -0.2483, -0.1949, -0.0061],\n",
       "                      [-0.3008, -0.0874,  0.1061, -0.4696,  0.2499, -0.1522,  0.0037,  0.0531,\n",
       "                       -0.2035,  0.1158, -0.2987,  0.1522,  0.5529, -0.0079, -0.1548,  0.1096]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.key.bias',\n",
       "              tensor([ 0.1556, -0.1500], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.values.weight',\n",
       "              tensor([[ 0.2386, -0.0009,  0.2135,  0.0763, -0.1546, -0.1292, -0.1162, -0.1615,\n",
       "                       -0.1834, -0.0030, -0.1196,  0.1610,  0.1356,  0.0502,  0.1142, -0.2218],\n",
       "                      [-0.2028, -0.2221,  0.2301,  0.1236, -0.0146, -0.2201,  0.0971,  0.1391,\n",
       "                       -0.0515,  0.0830,  0.1223, -0.1203, -0.0175, -0.0445,  0.1991, -0.0749]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.3.values.bias',\n",
       "              tensor([ 0.1149, -0.0845], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 0.0255, -0.1056, -0.2461, -0.2169,  0.0962, -0.0653, -0.0336,  0.2409,\n",
       "                       -0.0318, -0.0389,  0.1084, -0.0134, -0.3054, -0.0848,  0.1284, -0.1183],\n",
       "                      [-0.2132, -0.2016, -0.1649, -0.1378,  0.3056,  0.1686, -0.0853, -0.0671,\n",
       "                       -0.1782, -0.0594,  0.1698,  0.0550, -0.0164,  0.2762, -0.0968,  0.0213]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.query.bias',\n",
       "              tensor([0.0500, 0.0081], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.key.weight',\n",
       "              tensor([[ 0.2754, -0.1679, -0.1415,  0.0162, -0.1950, -0.1441,  0.1940,  0.2037,\n",
       "                       -0.1603,  0.2187,  0.1912,  0.0734,  0.0010, -0.2001, -0.1166,  0.1852],\n",
       "                      [ 0.1880, -0.1049, -0.1240,  0.2551, -0.2488,  0.1552, -0.1980,  0.0837,\n",
       "                       -0.2865,  0.1170, -0.0439,  0.0088,  0.1887,  0.0344,  0.1486,  0.0987]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.key.bias',\n",
       "              tensor([-0.0628,  0.2112], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.values.weight',\n",
       "              tensor([[-0.0749,  0.2044, -0.1384, -0.1648,  0.0175,  0.0863, -0.2132,  0.1540,\n",
       "                        0.0538, -0.1490,  0.1275,  0.1316, -0.1601, -0.0431, -0.0158, -0.1187],\n",
       "                      [ 0.2109,  0.0596,  0.0147, -0.0765,  0.0300, -0.1858, -0.0799, -0.2436,\n",
       "                       -0.1313,  0.0420, -0.0101,  0.1165,  0.3158,  0.0552, -0.0481,  0.1876]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.4.values.bias',\n",
       "              tensor([0.2301, 0.0845], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.3097, -0.0092,  0.0044, -0.5338,  0.0197, -0.2566, -0.0703,  0.4164,\n",
       "                        0.1104,  0.1838,  0.2103,  0.0139,  0.2053,  0.0144,  0.3748, -0.0540],\n",
       "                      [ 0.0908,  0.2220, -0.0945,  0.0072, -0.2203,  0.2509,  0.0390, -0.1010,\n",
       "                        0.0752, -0.0550, -0.1074,  0.1937,  0.2796, -0.3064, -0.1222,  0.0378]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.query.bias',\n",
       "              tensor([ 0.2146, -0.2088], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.key.weight',\n",
       "              tensor([[ 0.1418,  0.3585,  0.0503,  0.2820,  0.2195,  0.3960, -0.0524,  0.2595,\n",
       "                       -0.0194, -0.1012, -0.3003,  0.2013, -0.2271, -0.4030, -0.3448, -0.0664],\n",
       "                      [ 0.0187, -0.2444, -0.1031,  0.1203,  0.0726, -0.1645,  0.0699,  0.1210,\n",
       "                       -0.1675,  0.0301, -0.0207,  0.1222,  0.2091,  0.3401,  0.3087, -0.1007]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.key.bias',\n",
       "              tensor([ 0.0815, -0.1478], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.values.weight',\n",
       "              tensor([[-0.2059,  0.0746, -0.0367, -0.2639, -0.1520, -0.1151, -0.2578, -0.1111,\n",
       "                        0.1680,  0.1303, -0.0402,  0.0567, -0.2072, -0.0883,  0.3289, -0.2840],\n",
       "                      [-0.0280, -0.0876, -0.1161, -0.1893,  0.2193, -0.0672,  0.0871,  0.2086,\n",
       "                        0.1488,  0.1278, -0.0927,  0.1029, -0.1088, -0.2358,  0.2098,  0.0614]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.5.values.bias',\n",
       "              tensor([0.0474, 0.0740], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.query.weight',\n",
       "              tensor([[-0.2575, -0.1662, -0.3322, -0.1170,  0.2587,  0.0613, -0.2006,  0.0273,\n",
       "                        0.1155,  0.1111,  0.3216, -0.0886, -0.2304, -0.1403,  0.2231, -0.2379],\n",
       "                      [-0.2017, -0.0131, -0.3619, -0.3972,  0.0060, -0.2730,  0.1304,  0.2150,\n",
       "                        0.1470,  0.0850,  0.3305, -0.1981, -0.2143,  0.0518,  0.0081, -0.1872]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.query.bias',\n",
       "              tensor([0.0758, 0.2669], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.key.weight',\n",
       "              tensor([[-0.1770,  0.3287, -0.1340, -0.2685,  0.0725,  0.1367, -0.2646,  0.3615,\n",
       "                        0.2618,  0.0284,  0.0752, -0.0890,  0.1398, -0.0891, -0.1490, -0.1616],\n",
       "                      [-0.2287,  0.2579,  0.2551,  0.0720,  0.0615,  0.3344, -0.1602,  0.3185,\n",
       "                        0.2251, -0.1123, -0.2966,  0.2503,  0.2355, -0.2077,  0.1301, -0.2090]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.key.bias',\n",
       "              tensor([0.1126, 0.1032], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.values.weight',\n",
       "              tensor([[-0.0764, -0.1261,  0.1120, -0.2195,  0.2046,  0.0115, -0.1380, -0.0308,\n",
       "                        0.3271, -0.0773, -0.1965,  0.2553,  0.1703,  0.0807, -0.0896, -0.0321],\n",
       "                      [ 0.1301, -0.0034,  0.2501, -0.0399,  0.0906,  0.1657,  0.0620,  0.1251,\n",
       "                        0.1891, -0.1974,  0.1924, -0.0970,  0.2722,  0.1648, -0.1579, -0.1721]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.6.values.bias',\n",
       "              tensor([0.1197, 0.0639], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.query.weight',\n",
       "              tensor([[ 0.1648,  0.4128,  0.0915,  0.2838, -0.3200, -0.0097,  0.2129,  0.1871,\n",
       "                        0.1709, -0.2496, -0.3959, -0.1253,  0.2893, -0.3752,  0.0900,  0.0522],\n",
       "                      [-0.5484, -0.2004, -0.4375, -0.5880,  0.0781, -0.3539,  0.1520, -0.1213,\n",
       "                        0.2390,  0.3726,  0.3703,  0.1136, -0.3846,  0.3959, -0.0404,  0.3299]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.query.bias',\n",
       "              tensor([-0.1478,  0.4403], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.key.weight',\n",
       "              tensor([[ 0.0662, -0.3058, -0.0165,  0.2200,  0.0779, -0.3005, -0.0399, -0.0432,\n",
       "                       -0.2860,  0.1617,  0.3949, -0.2666,  0.2216,  0.4804, -0.2949,  0.2398],\n",
       "                      [-0.0743,  0.5220, -0.0020,  0.1441,  0.0895,  0.5056, -0.0769,  0.3539,\n",
       "                        0.0608, -0.3608, -0.4723,  0.0717, -0.1865, -0.5372,  0.3026, -0.5014]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.key.bias',\n",
       "              tensor([ 0.2305, -0.1531], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.values.weight',\n",
       "              tensor([[-0.2137, -0.0138, -0.0720,  0.0085,  0.1205, -0.2258,  0.2561,  0.1250,\n",
       "                        0.1251,  0.0036,  0.1779, -0.0075, -0.2514, -0.1898,  0.2907, -0.0424],\n",
       "                      [ 0.2091,  0.0167, -0.0388,  0.0473, -0.2579,  0.2267, -0.0163, -0.0602,\n",
       "                       -0.1746,  0.1882,  0.2702, -0.0923,  0.0716,  0.1212, -0.2287, -0.0331]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.heads.7.values.bias',\n",
       "              tensor([-0.2225, -0.1477], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.proj.weight',\n",
       "              tensor([[ 2.1762e-02, -5.4363e-02,  1.9535e-01,  1.0807e-01,  1.8185e-01,\n",
       "                        8.4266e-02,  1.1481e-01,  8.5615e-03,  1.8424e-01, -1.8966e-01,\n",
       "                        1.0922e-01, -2.0082e-02, -1.5612e-01,  2.3181e-01,  2.1352e-01,\n",
       "                       -2.4351e-01],\n",
       "                      [ 4.5451e-03, -3.3017e-02, -1.5648e-01, -1.5126e-02,  1.3219e-01,\n",
       "                       -1.8238e-01, -1.9689e-01, -5.4763e-02,  2.0511e-01,  2.4299e-01,\n",
       "                       -1.4564e-01, -1.9861e-01, -2.2089e-01, -1.1970e-01, -1.7560e-01,\n",
       "                        1.4667e-01],\n",
       "                      [-2.8843e-01,  1.2518e-02,  2.4363e-01, -1.8766e-01,  1.9765e-01,\n",
       "                       -2.1449e-01, -1.4473e-01, -7.8899e-02,  7.3245e-03,  7.0904e-02,\n",
       "                       -9.5328e-02,  1.0640e-01, -1.9357e-01, -1.3319e-01,  3.9263e-02,\n",
       "                        1.2299e-01],\n",
       "                      [-1.0604e-01,  1.7757e-01,  2.7407e-01, -3.7871e-03,  1.6299e-01,\n",
       "                        2.9028e-01, -1.5493e-03, -8.5497e-02,  1.2810e-01,  1.2760e-01,\n",
       "                        2.5439e-01, -2.1783e-01, -1.0007e-01, -9.4088e-02, -1.4249e-01,\n",
       "                       -4.3217e-02],\n",
       "                      [-1.7995e-01, -3.6519e-02,  8.8822e-02, -2.8279e-02, -3.3878e-02,\n",
       "                       -1.0354e-02, -7.6558e-03,  6.2549e-04,  4.0838e-02, -1.3995e-01,\n",
       "                       -2.7020e-01,  1.1124e-01, -2.8118e-01, -2.7526e-01, -1.9935e-01,\n",
       "                        2.2124e-01],\n",
       "                      [ 2.0365e-01, -2.4818e-01,  1.1638e-01,  1.5142e-01, -6.6850e-02,\n",
       "                       -1.9825e-01,  7.3642e-02, -1.9632e-01,  1.7591e-01,  1.7845e-01,\n",
       "                        2.6690e-01,  2.4196e-01, -1.6189e-01, -1.6289e-01, -8.5606e-02,\n",
       "                       -5.5670e-02],\n",
       "                      [ 2.6609e-01,  2.8782e-01,  1.9773e-01, -3.0424e-02,  1.6461e-01,\n",
       "                       -1.1989e-01, -1.3348e-01,  1.4070e-01,  1.4520e-01, -9.2381e-02,\n",
       "                        2.0776e-01, -2.2109e-01,  1.1703e-01,  3.3967e-02,  1.6337e-01,\n",
       "                       -2.1726e-01],\n",
       "                      [ 9.5731e-03,  3.1732e-01, -1.6860e-01,  8.8076e-02, -2.8394e-01,\n",
       "                        1.1499e-02, -1.3309e-01,  1.3715e-01, -1.0873e-01, -2.4710e-01,\n",
       "                       -1.2240e-01,  5.4646e-02,  3.3879e-01,  2.8778e-01, -1.8057e-01,\n",
       "                       -2.6414e-01],\n",
       "                      [ 1.2366e-01,  1.2602e-01,  3.2341e-02, -7.1459e-02,  2.0032e-01,\n",
       "                        1.6132e-01, -6.7811e-02,  1.6910e-01, -2.4402e-01,  1.1765e-01,\n",
       "                       -1.9443e-01, -1.2381e-01,  2.5905e-01,  1.0897e-01,  1.3217e-01,\n",
       "                        1.4262e-01],\n",
       "                      [ 1.9044e-01, -7.7480e-02, -2.2360e-01,  5.5822e-02,  2.6943e-01,\n",
       "                       -7.9128e-02,  1.7043e-01,  2.6704e-02,  1.5590e-01, -1.3391e-01,\n",
       "                        2.2670e-01,  5.9621e-02,  1.2979e-01,  1.7950e-01,  2.4869e-01,\n",
       "                        1.1466e-01],\n",
       "                      [ 1.0414e-01, -1.6227e-01,  1.1655e-01,  1.2714e-01,  9.5499e-02,\n",
       "                       -1.5456e-01, -1.2560e-01,  1.9487e-01, -1.6435e-01, -8.8591e-02,\n",
       "                        1.3717e-01,  4.9934e-02,  1.4827e-01,  9.3184e-02,  3.9529e-02,\n",
       "                        1.1764e-01],\n",
       "                      [ 3.4989e-02, -1.9019e-01,  8.2675e-02, -2.4403e-01,  2.3753e-01,\n",
       "                        5.9376e-02,  1.8632e-01,  1.3312e-01, -1.1904e-01,  2.5138e-01,\n",
       "                        2.0440e-01,  2.2788e-01, -7.1492e-02,  2.0903e-01, -6.5699e-02,\n",
       "                        2.0542e-01],\n",
       "                      [-2.1151e-01,  9.6987e-02, -2.5452e-01, -1.1857e-01,  4.2442e-02,\n",
       "                       -1.6895e-01,  6.3190e-02, -1.4948e-01,  2.0696e-01, -9.7008e-02,\n",
       "                       -9.1932e-02, -6.5631e-02, -3.0176e-01, -1.4665e-01,  2.1385e-01,\n",
       "                        2.7841e-01],\n",
       "                      [ 1.3374e-01,  1.8494e-01,  2.2570e-01, -6.5546e-02,  1.7352e-02,\n",
       "                        2.7083e-01, -7.7217e-02, -1.2623e-02,  7.0041e-02,  1.1796e-01,\n",
       "                       -3.5850e-02,  1.9871e-01,  5.2685e-02, -2.5685e-02,  6.9246e-02,\n",
       "                        2.0875e-02],\n",
       "                      [-1.5950e-01,  5.9955e-02,  4.3651e-02,  1.2070e-04, -4.7782e-02,\n",
       "                        7.5437e-02,  2.0180e-01, -1.8340e-01,  1.4214e-01,  1.8882e-02,\n",
       "                        1.3648e-01,  2.3129e-02, -2.8764e-01, -2.8855e-01,  1.1284e-01,\n",
       "                        1.1145e-01],\n",
       "                      [ 5.9660e-02,  5.4308e-02,  2.1435e-01,  1.8218e-01, -5.1264e-02,\n",
       "                       -2.5656e-03,  5.5456e-02, -1.3139e-01, -1.8909e-01, -2.4120e-01,\n",
       "                       -9.8704e-02, -1.0653e-01, -2.7642e-01, -2.3324e-01, -1.5921e-01,\n",
       "                        3.3765e-01]], device='cuda:0')),\n",
       "             ('tab_layers.3.self_attention.proj.bias',\n",
       "              tensor([-0.1258, -0.0881, -0.0508,  0.0982,  0.1381,  0.1470, -0.1303,  0.2027,\n",
       "                       0.0830,  0.0309,  0.2312, -0.2001,  0.0498,  0.1619,  0.0557,  0.1926],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.layer_norm2.weight',\n",
       "              tensor([1.0160, 0.9986, 0.9909, 1.0284, 1.0144, 1.0033, 0.9967, 1.0631, 1.0327,\n",
       "                      0.9992, 0.9787, 0.9924, 1.0110, 1.0244, 1.0080, 1.0440],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.layer_norm2.bias',\n",
       "              tensor([-0.0052,  0.0008,  0.0031,  0.0046,  0.0025, -0.0079, -0.0127,  0.0089,\n",
       "                      -0.0049,  0.0048, -0.0102, -0.0075,  0.0060,  0.0042,  0.0114,  0.0013],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.mlp.mlp.0.weight',\n",
       "              tensor([[ 0.1769, -0.0478,  0.0398,  ..., -0.1463,  0.1141,  0.1253],\n",
       "                      [ 0.0373, -0.0196,  0.0685,  ..., -0.0222, -0.1942,  0.2063],\n",
       "                      [ 0.1929, -0.0404, -0.1379,  ..., -0.1912, -0.2136,  0.1776],\n",
       "                      ...,\n",
       "                      [ 0.0273, -0.0956, -0.0119,  ..., -0.1898,  0.0949,  0.3138],\n",
       "                      [-0.3006, -0.0821,  0.2752,  ..., -0.0070, -0.0970,  0.1619],\n",
       "                      [ 0.0448, -0.2526, -0.0701,  ..., -0.0472, -0.3423, -0.1192]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.mlp.mlp.0.bias',\n",
       "              tensor([-0.2093,  0.0627, -0.1999, -0.0629, -0.0378,  0.0323,  0.0787,  0.0918,\n",
       "                       0.0867, -0.1509,  0.2622,  0.1646,  0.1410, -0.0443,  0.1962, -0.1127,\n",
       "                      -0.1960, -0.0185,  0.2398,  0.0541, -0.0850, -0.0893, -0.0554, -0.0987,\n",
       "                      -0.1088,  0.2275,  0.0984, -0.0072,  0.1977,  0.0150,  0.0747,  0.0112,\n",
       "                      -0.1859, -0.2128, -0.0789, -0.1724,  0.2058,  0.1523,  0.2047, -0.2459,\n",
       "                      -0.1176, -0.1760, -0.0749, -0.2075, -0.0094,  0.0954,  0.1149,  0.2087,\n",
       "                      -0.1095,  0.1803, -0.1692, -0.0587, -0.0088, -0.1122, -0.0337, -0.2448,\n",
       "                       0.1044, -0.1043,  0.2450, -0.2257,  0.0766, -0.0102,  0.1156,  0.1087],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.mlp.mlp.2.weight',\n",
       "              tensor([[ 0.0907, -0.0907,  0.0056,  ..., -0.0299,  0.0249, -0.0778],\n",
       "                      [ 0.0827,  0.0408, -0.0162,  ...,  0.0907,  0.1814,  0.1359],\n",
       "                      [-0.0577,  0.0770, -0.0530,  ..., -0.0311, -0.0303, -0.1002],\n",
       "                      ...,\n",
       "                      [ 0.0688, -0.0406,  0.0773,  ..., -0.0164,  0.0252, -0.2032],\n",
       "                      [ 0.0594, -0.1209, -0.0392,  ..., -0.0365, -0.0845, -0.0710],\n",
       "                      [-0.0825, -0.1039, -0.0239,  ..., -0.0978, -0.0007,  0.0050]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.3.mlp.mlp.2.bias',\n",
       "              tensor([ 0.0127, -0.1147,  0.0562, -0.0032, -0.0977, -0.0204,  0.1059,  0.0461,\n",
       "                       0.1100, -0.1182,  0.0255,  0.0317, -0.0692, -0.0964, -0.0981,  0.0438],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.layer_norm1.weight',\n",
       "              tensor([1.0575, 1.0421, 0.9845, 1.0671, 1.1938, 1.0036, 1.1286, 1.0742, 1.0577,\n",
       "                      1.0439, 0.9972, 1.0153, 1.0474, 1.0473, 1.0043, 1.0222],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.layer_norm1.bias',\n",
       "              tensor([-0.0301, -0.0030, -0.0007,  0.0049,  0.0034, -0.0053, -0.0011,  0.0400,\n",
       "                      -0.0056,  0.0107,  0.0134, -0.0159, -0.0108,  0.0264,  0.0109, -0.0032],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.query.weight',\n",
       "              tensor([[-2.7299e-01,  1.4574e-02, -2.6538e-01, -2.3598e-01, -1.8673e-01,\n",
       "                        4.0416e-02,  2.9535e-01,  4.3350e-04,  3.0257e-02,  4.0970e-01,\n",
       "                        3.2799e-02, -3.4832e-01, -3.1612e-01,  3.5010e-01,  3.5366e-01,\n",
       "                        1.5303e-01],\n",
       "                      [ 4.3175e-02,  1.6205e-02, -1.5365e-01, -4.6788e-01,  4.1818e-01,\n",
       "                       -1.5180e-01,  9.8181e-03,  2.6007e-01, -4.1660e-01,  3.6855e-01,\n",
       "                       -8.5253e-02, -2.1764e-01, -3.2014e-02,  1.7014e-01,  8.9535e-02,\n",
       "                        4.8395e-02]], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.query.bias',\n",
       "              tensor([0.0804, 0.0103], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.key.weight',\n",
       "              tensor([[ 0.2444, -0.0472,  0.2131, -0.1680, -0.2266,  0.4255,  0.1770, -0.0181,\n",
       "                        0.1852, -0.1864,  0.0300, -0.0518, -0.2403, -0.3493, -0.4687,  0.1448],\n",
       "                      [-0.3297,  0.0548, -0.0111, -0.0423, -0.0761,  0.1797, -0.3580,  0.1611,\n",
       "                        0.4224,  0.0822, -0.1604,  0.1252,  0.1788, -0.2240, -0.1827, -0.0385]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.key.bias',\n",
       "              tensor([ 0.0539, -0.1352], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.values.weight',\n",
       "              tensor([[ 0.0548, -0.0609,  0.2295,  0.0506,  0.1092, -0.0215,  0.0274, -0.1538,\n",
       "                       -0.1489,  0.2516, -0.0893,  0.2303,  0.0163, -0.0744,  0.2466,  0.1646],\n",
       "                      [-0.1206, -0.2068, -0.2040, -0.1020,  0.1814,  0.1811, -0.0818, -0.0591,\n",
       "                       -0.1449,  0.1324,  0.1571,  0.2547,  0.1524, -0.1024,  0.0097,  0.0596]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.0.values.bias',\n",
       "              tensor([ 0.1300, -0.1043], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.1480,  0.2210,  0.0893,  0.1157, -0.0459, -0.0974, -0.3368, -0.0512,\n",
       "                       -0.0842, -0.2507, -0.3026,  0.1017,  0.0598, -0.2103, -0.2082,  0.0965],\n",
       "                      [-0.3292, -0.3601, -0.0110, -0.1392, -0.0216, -0.2114,  0.0989,  0.1320,\n",
       "                       -0.0037,  0.3524,  0.2174, -0.3725, -0.4080,  0.2847,  0.2141, -0.1667]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.query.bias',\n",
       "              tensor([-0.3621,  0.1666], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.key.weight',\n",
       "              tensor([[-0.0025, -0.4128, -0.1437,  0.2368, -0.3097,  0.1036,  0.2734, -0.1076,\n",
       "                       -0.1505,  0.2732,  0.2414, -0.0336,  0.0596,  0.0821, -0.0496, -0.2936],\n",
       "                      [-0.3825,  0.3246,  0.0359,  0.0828,  0.4230,  0.0804, -0.3773, -0.3336,\n",
       "                        0.1644, -0.1353, -0.2469,  0.1831, -0.0988, -0.3918, -0.0062, -0.1698]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.key.bias',\n",
       "              tensor([0.2731, 0.0064], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.values.weight',\n",
       "              tensor([[ 0.1357, -0.1647, -0.0293,  0.1997, -0.0573,  0.1770,  0.0832,  0.1725,\n",
       "                       -0.3078,  0.2105,  0.1010, -0.1114, -0.0100,  0.1380,  0.1710, -0.0546],\n",
       "                      [ 0.1604,  0.2477,  0.1704, -0.0172,  0.0303,  0.0310, -0.1981,  0.1756,\n",
       "                        0.3213, -0.3196,  0.1811,  0.1363,  0.0942, -0.1178, -0.0545,  0.1885]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.1.values.bias',\n",
       "              tensor([ 0.2147, -0.0753], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.query.weight',\n",
       "              tensor([[ 0.0641, -0.1115, -0.2142, -0.1955, -0.3421, -0.1033,  0.2398,  0.1261,\n",
       "                       -0.1479,  0.2152,  0.2945, -0.1127, -0.2287, -0.0220, -0.0406, -0.2718],\n",
       "                      [ 0.0438,  0.0756, -0.1058,  0.1178, -0.2026,  0.1024, -0.0668,  0.1413,\n",
       "                        0.2056,  0.2874,  0.0811, -0.2455, -0.1998, -0.1523,  0.2428, -0.1234]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.query.bias',\n",
       "              tensor([0.1272, 0.3092], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.key.weight',\n",
       "              tensor([[ 0.1962,  0.1748, -0.0740,  0.2209,  0.0388,  0.1291,  0.1777, -0.1452,\n",
       "                       -0.2207, -0.2942, -0.2261, -0.2605, -0.2730, -0.1532, -0.1093,  0.1711],\n",
       "                      [ 0.1137,  0.0945,  0.0137,  0.2252, -0.3559, -0.0567,  0.1066, -0.1355,\n",
       "                       -0.1587,  0.0819,  0.1528, -0.1809,  0.0801, -0.1461, -0.0702, -0.1534]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.key.bias',\n",
       "              tensor([-0.0907, -0.0293], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.values.weight',\n",
       "              tensor([[ 0.1515,  0.1837,  0.1860, -0.0022, -0.2944, -0.1711, -0.1272,  0.2095,\n",
       "                       -0.2262, -0.1298, -0.0170, -0.2487, -0.1585, -0.0829,  0.0350, -0.0076],\n",
       "                      [ 0.0894,  0.0469,  0.1789, -0.1526, -0.2031, -0.2239, -0.0293,  0.0177,\n",
       "                       -0.2413,  0.0617, -0.1932, -0.1752, -0.1432, -0.1237,  0.1947, -0.1315]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.2.values.bias',\n",
       "              tensor([-0.0778,  0.1939], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.query.weight',\n",
       "              tensor([[-0.3911, -0.2744,  0.0825, -0.1871, -0.0381, -0.3691,  0.1223,  0.0055,\n",
       "                       -0.0242,  0.3681,  0.2832, -0.2733, -0.3058,  0.3180,  0.1928,  0.2244],\n",
       "                      [ 0.2914,  0.3277, -0.0154,  0.4529,  0.2812,  0.2759, -0.0064, -0.2563,\n",
       "                        0.2962, -0.2453, -0.2955,  0.3481,  0.3483, -0.1119, -0.0677, -0.0462]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.query.bias',\n",
       "              tensor([ 0.2407, -0.0739], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.key.weight',\n",
       "              tensor([[-0.0535,  0.3508,  0.0553, -0.0546, -0.0657,  0.3599, -0.2416,  0.1681,\n",
       "                        0.0360, -0.2409,  0.0343, -0.1111, -0.1604, -0.1454, -0.2245, -0.1519],\n",
       "                      [ 0.0440, -0.3838, -0.2875, -0.2538, -0.0661, -0.2065,  0.1086, -0.1868,\n",
       "                       -0.2646,  0.2119,  0.0461,  0.2113,  0.1319,  0.2393,  0.3623, -0.0008]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.key.bias',\n",
       "              tensor([-0.1513,  0.1678], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.values.weight',\n",
       "              tensor([[ 0.0548, -0.0546, -0.0105,  0.1919,  0.3112,  0.1487, -0.2573,  0.2460,\n",
       "                        0.0663,  0.2331,  0.1052, -0.0609,  0.1785,  0.2429,  0.2773, -0.0455],\n",
       "                      [-0.3213, -0.2626,  0.1377, -0.0227,  0.2802, -0.1455, -0.1427,  0.2022,\n",
       "                       -0.1103, -0.2393, -0.2036,  0.0256,  0.0631,  0.0478,  0.0477,  0.0069]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.3.values.bias',\n",
       "              tensor([-0.2247,  0.2239], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 0.2257,  0.0968,  0.2555,  0.2004,  0.0886,  0.1406, -0.0667, -0.1623,\n",
       "                       -0.0310, -0.3000, -0.0593,  0.1302, -0.1517, -0.0582, -0.1716,  0.1084],\n",
       "                      [ 0.3587,  0.2568,  0.0397,  0.2444,  0.2518,  0.2531,  0.1260, -0.1643,\n",
       "                        0.3037, -0.0788, -0.1337,  0.2628,  0.0956, -0.1983,  0.0048, -0.0711]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.query.bias',\n",
       "              tensor([-0.2383, -0.2353], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.key.weight',\n",
       "              tensor([[ 0.2013, -0.0871, -0.1997,  0.0256, -0.3620,  0.2087, -0.0535,  0.2653,\n",
       "                       -0.1919, -0.2592, -0.0034, -0.2747, -0.1605,  0.0120,  0.1180, -0.2782],\n",
       "                      [ 0.2172, -0.0952, -0.0215,  0.0254, -0.1701,  0.2868,  0.1119, -0.0285,\n",
       "                        0.0517,  0.1264,  0.1120,  0.0164, -0.2250,  0.0216, -0.1223, -0.2789]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.key.bias',\n",
       "              tensor([-0.2020, -0.0016], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.values.weight',\n",
       "              tensor([[ 0.0890, -0.1186, -0.1981,  0.0887, -0.0628,  0.2227,  0.0277, -0.1076,\n",
       "                        0.0214, -0.1921, -0.0913,  0.2687,  0.0026, -0.1082, -0.2089,  0.2407],\n",
       "                      [ 0.0880,  0.0375, -0.0627, -0.0589,  0.1354,  0.2036, -0.0659, -0.1658,\n",
       "                        0.1776,  0.0008,  0.1268, -0.0929,  0.1452, -0.1055, -0.0972,  0.0163]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.4.values.bias',\n",
       "              tensor([-0.0739, -0.1737], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.0816, -0.0995,  0.0330,  0.2852,  0.1048,  0.1040, -0.2239, -0.2236,\n",
       "                       -0.1784,  0.0676, -0.0970, -0.0185,  0.0363, -0.2624,  0.1075,  0.1485],\n",
       "                      [ 0.0044,  0.3073, -0.0568,  0.0391, -0.0642,  0.3332, -0.1244, -0.1696,\n",
       "                        0.2204, -0.3449, -0.2538,  0.2519,  0.0235, -0.0575, -0.1461,  0.0128]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.query.bias',\n",
       "              tensor([-0.0775, -0.2651], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.key.weight',\n",
       "              tensor([[-0.0658, -0.2658, -0.2101, -0.2581,  0.0221, -0.0677, -0.0274,  0.2637,\n",
       "                       -0.2155,  0.0749,  0.2758,  0.1512,  0.1312, -0.0080, -0.1568,  0.1525],\n",
       "                      [ 0.1527, -0.2198, -0.0865, -0.1336, -0.2444, -0.0917,  0.1769,  0.1577,\n",
       "                       -0.2479,  0.1863, -0.0635, -0.1826, -0.0154,  0.3929, -0.0067,  0.2595]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.key.bias',\n",
       "              tensor([0.2046, 0.1719], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.values.weight',\n",
       "              tensor([[-0.1574,  0.1460,  0.1125,  0.1011,  0.0984, -0.1324,  0.1714, -0.1315,\n",
       "                        0.2920, -0.0701, -0.1610, -0.1576,  0.0683, -0.1748, -0.0959, -0.2022],\n",
       "                      [-0.1144, -0.0251, -0.1237, -0.1367,  0.0986,  0.2145, -0.1123,  0.1882,\n",
       "                       -0.0059, -0.1497, -0.1846,  0.0427, -0.0598,  0.0744,  0.1628,  0.1934]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.5.values.bias',\n",
       "              tensor([ 0.2445, -0.1263], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.query.weight',\n",
       "              tensor([[ 0.0149,  0.1046, -0.1824, -0.2990,  0.1336, -0.1620, -0.1777, -0.0939,\n",
       "                       -0.1658,  0.1290,  0.2323, -0.0699,  0.2555,  0.2532, -0.0246, -0.0711],\n",
       "                      [ 0.1599, -0.1673,  0.0578,  0.2171,  0.0046, -0.1302, -0.0130,  0.2239,\n",
       "                       -0.1068, -0.0439, -0.2855,  0.0374, -0.2574,  0.0084,  0.0121,  0.0279]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.query.bias',\n",
       "              tensor([ 0.1759, -0.1307], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.key.weight',\n",
       "              tensor([[-0.2829, -0.1354, -0.1348, -0.1275,  0.2341,  0.1470, -0.0769, -0.1388,\n",
       "                       -0.1730, -0.2392,  0.0640,  0.0997,  0.1444,  0.1685, -0.2196, -0.0581],\n",
       "                      [ 0.2372,  0.0632,  0.0010,  0.2405, -0.1300,  0.0533,  0.2490,  0.1088,\n",
       "                        0.0261,  0.1046,  0.0277, -0.1378,  0.1224, -0.2665, -0.1136,  0.1077]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.key.bias',\n",
       "              tensor([ 0.0829, -0.1409], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.values.weight',\n",
       "              tensor([[ 0.1921,  0.1760, -0.0195, -0.0288,  0.1596, -0.0117,  0.1762, -0.1676,\n",
       "                       -0.0840, -0.0589, -0.0896, -0.1616, -0.1727, -0.2109, -0.0114, -0.0771],\n",
       "                      [-0.0964,  0.1198,  0.1333, -0.2470, -0.2052,  0.1034, -0.2110, -0.1777,\n",
       "                       -0.2127,  0.0498,  0.0540,  0.1208, -0.0465, -0.2441,  0.0553,  0.0207]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.6.values.bias',\n",
       "              tensor([ 0.2369, -0.0418], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.query.weight',\n",
       "              tensor([[ 0.3111,  0.0629,  0.0626, -0.0722,  0.3169, -0.0897, -0.3022, -0.0433,\n",
       "                        0.1804, -0.2259, -0.1957, -0.1345,  0.0631, -0.2659, -0.0122, -0.0528],\n",
       "                      [ 0.1564,  0.1679,  0.0526, -0.0608,  0.0059,  0.0762, -0.2784, -0.2438,\n",
       "                        0.0136, -0.1801, -0.3137,  0.3193, -0.0763, -0.3651, -0.2708, -0.0018]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.query.bias',\n",
       "              tensor([-0.2311, -0.1401], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.key.weight',\n",
       "              tensor([[-0.3558, -0.1944,  0.1293, -0.2628,  0.1257, -0.3926, -0.0061, -0.0592,\n",
       "                       -0.0404,  0.2958,  0.2967,  0.2270,  0.3019,  0.0882,  0.0014,  0.2410],\n",
       "                      [-0.2410, -0.3534, -0.1804, -0.1213,  0.3392, -0.3338, -0.0438, -0.3277,\n",
       "                       -0.0413,  0.2333, -0.0639,  0.0247,  0.1596,  0.2644, -0.0843, -0.0197]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.key.bias',\n",
       "              tensor([-0.0207,  0.0435], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.values.weight',\n",
       "              tensor([[ 0.0887,  0.0037, -0.0557, -0.3112,  0.0472, -0.1039, -0.2178,  0.1425,\n",
       "                        0.1051,  0.0755, -0.2191,  0.0843,  0.1403,  0.2595, -0.0253,  0.1124],\n",
       "                      [ 0.0792,  0.2346,  0.0220,  0.2872, -0.3654,  0.1460, -0.1685,  0.2291,\n",
       "                        0.1549,  0.0189, -0.0106,  0.1242, -0.0876, -0.0474,  0.0234,  0.0150]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.heads.7.values.bias',\n",
       "              tensor([-0.1883, -0.1786], device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.proj.weight',\n",
       "              tensor([[-0.0487,  0.1544, -0.1781,  0.0434,  0.2103,  0.2522,  0.1842,  0.0870,\n",
       "                       -0.0050,  0.2020, -0.1312, -0.1750, -0.0064,  0.1642, -0.0398, -0.1343],\n",
       "                      [ 0.1089,  0.0252, -0.1688,  0.1274,  0.1778, -0.0256, -0.2341, -0.3416,\n",
       "                       -0.0246, -0.0702, -0.0257,  0.1191,  0.0206, -0.1814, -0.3373,  0.2790],\n",
       "                      [-0.2331,  0.1673,  0.2268, -0.1049, -0.0797,  0.0456, -0.3535, -0.1675,\n",
       "                       -0.1836,  0.0782, -0.1245, -0.2355, -0.1697, -0.2470, -0.1529,  0.0031],\n",
       "                      [ 0.0467, -0.0127, -0.3975,  0.1685,  0.0845,  0.2166,  0.0654,  0.0869,\n",
       "                        0.0936, -0.2627,  0.1801,  0.1566, -0.1257,  0.0323, -0.0475, -0.2856],\n",
       "                      [-0.2241, -0.1221,  0.1804, -0.2066,  0.1802, -0.0560, -0.2123,  0.1249,\n",
       "                       -0.0095, -0.2125, -0.1106,  0.0420, -0.0716, -0.2483, -0.0108, -0.0278],\n",
       "                      [ 0.2316,  0.0822, -0.1220, -0.0763, -0.1554,  0.2221, -0.1171, -0.1447,\n",
       "                       -0.1921, -0.1533, -0.2313,  0.1212,  0.2381,  0.0875,  0.0096, -0.1180],\n",
       "                      [-0.1622,  0.2004, -0.1582,  0.3558, -0.1359, -0.1274,  0.0248,  0.1047,\n",
       "                        0.2031, -0.0831,  0.2731,  0.1175,  0.0034, -0.0891,  0.0396,  0.2170],\n",
       "                      [ 0.0342, -0.1628, -0.0374,  0.0320, -0.0532, -0.2741,  0.1480,  0.1126,\n",
       "                        0.0475, -0.0376, -0.0015,  0.0168,  0.2062,  0.0483,  0.0854,  0.0414],\n",
       "                      [ 0.0357,  0.1664, -0.0667, -0.2137,  0.2439, -0.1675, -0.0539, -0.0214,\n",
       "                       -0.2057, -0.1331, -0.1626,  0.0850,  0.0711, -0.0422, -0.1601, -0.2364],\n",
       "                      [ 0.1595, -0.1209,  0.1630,  0.0954, -0.0168, -0.0729, -0.0417, -0.0073,\n",
       "                       -0.0041, -0.0823, -0.1779,  0.1862,  0.1233,  0.0779, -0.1919, -0.1586],\n",
       "                      [ 0.1027,  0.2632, -0.1260,  0.1675, -0.1625, -0.1368,  0.1786,  0.0516,\n",
       "                        0.0521, -0.0005,  0.0802, -0.0603,  0.0930,  0.1912,  0.1608, -0.0881],\n",
       "                      [ 0.2267,  0.1606,  0.0595,  0.0382, -0.0847, -0.1648, -0.1851,  0.1042,\n",
       "                        0.0500,  0.0305,  0.0780,  0.1111, -0.1553, -0.2382,  0.1103, -0.2112],\n",
       "                      [ 0.0564,  0.1124, -0.0169,  0.0703, -0.2135,  0.1840, -0.0692,  0.2184,\n",
       "                       -0.0631,  0.1875,  0.2211,  0.2304,  0.1977,  0.0353,  0.0317,  0.2429],\n",
       "                      [ 0.1902, -0.0490, -0.2512, -0.1150, -0.2520,  0.2379,  0.1446,  0.2201,\n",
       "                        0.1323,  0.1339,  0.0618,  0.2228,  0.0392,  0.1595,  0.0661,  0.0548],\n",
       "                      [-0.0776, -0.2493,  0.2676, -0.1895,  0.1994,  0.1397, -0.1385, -0.1630,\n",
       "                        0.1427, -0.0501, -0.0287, -0.0086, -0.0111,  0.0582, -0.2377, -0.1433],\n",
       "                      [-0.2509,  0.0247,  0.1262, -0.0815, -0.0831,  0.1565, -0.1862, -0.1641,\n",
       "                       -0.1284, -0.2033,  0.0986, -0.2403,  0.0087, -0.2315, -0.1575,  0.0594]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.self_attention.proj.bias',\n",
       "              tensor([ 0.1637,  0.2512,  0.0535, -0.1935, -0.0592, -0.2081, -0.2148, -0.2285,\n",
       "                      -0.0911,  0.0871,  0.0376,  0.2001,  0.2522,  0.1233,  0.1582, -0.2214],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.layer_norm2.weight',\n",
       "              tensor([1.0093, 1.0030, 1.0182, 0.9788, 1.0138, 0.9956, 1.0158, 1.0044, 1.0092,\n",
       "                      0.9893, 0.9930, 0.9778, 0.9873, 1.0142, 1.0093, 1.0295],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.layer_norm2.bias',\n",
       "              tensor([ 0.0068,  0.0096, -0.0018,  0.0217,  0.0050,  0.0037, -0.0177,  0.0048,\n",
       "                      -0.0086, -0.0078, -0.0070, -0.0022,  0.0086,  0.0025, -0.0093,  0.0013],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.mlp.mlp.0.weight',\n",
       "              tensor([[-0.0714, -0.0666, -0.2011,  ...,  0.0427, -0.0846, -0.0232],\n",
       "                      [ 0.1751,  0.2309,  0.2566,  ...,  0.1162,  0.2977, -0.1699],\n",
       "                      [ 0.2379, -0.0324, -0.2005,  ..., -0.0027,  0.0517,  0.1495],\n",
       "                      ...,\n",
       "                      [ 0.0985, -0.1428,  0.1672,  ..., -0.1337, -0.0641, -0.1785],\n",
       "                      [ 0.0783, -0.0365, -0.2592,  ...,  0.0826, -0.2338, -0.1698],\n",
       "                      [ 0.0767, -0.0087,  0.0281,  ...,  0.0053, -0.2204, -0.2145]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.mlp.mlp.0.bias',\n",
       "              tensor([-0.0374, -0.0109, -0.0359,  0.0435, -0.0837, -0.0875,  0.0639, -0.0979,\n",
       "                       0.2205, -0.1806, -0.0430,  0.0632,  0.0674,  0.0321, -0.0046, -0.1857,\n",
       "                       0.1588,  0.0062,  0.1257, -0.0206,  0.0948, -0.1135, -0.0249,  0.0078,\n",
       "                       0.1026,  0.0403,  0.0227, -0.1346, -0.0787, -0.0652,  0.1545,  0.1744,\n",
       "                      -0.0308, -0.1649, -0.1609, -0.1990,  0.0704,  0.2204,  0.2674,  0.2480,\n",
       "                      -0.1176,  0.2064,  0.1193,  0.0407,  0.0596, -0.1423, -0.0262, -0.2225,\n",
       "                       0.0051,  0.2495,  0.1564, -0.1034, -0.0217,  0.2557, -0.0514,  0.1553,\n",
       "                       0.1597,  0.0329, -0.1582,  0.0044, -0.2103, -0.1281,  0.0024,  0.0289],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.mlp.mlp.2.weight',\n",
       "              tensor([[-0.1150,  0.0678,  0.0786,  ..., -0.0374,  0.0472,  0.0975],\n",
       "                      [-0.0134,  0.1241, -0.0307,  ...,  0.0729, -0.0643, -0.1480],\n",
       "                      [ 0.0272, -0.0867,  0.0867,  ..., -0.0374, -0.0647, -0.0474],\n",
       "                      ...,\n",
       "                      [-0.0866,  0.0070,  0.0607,  ...,  0.0269,  0.0321,  0.0781],\n",
       "                      [ 0.0982,  0.0370,  0.0764,  ..., -0.0646, -0.0172, -0.0257],\n",
       "                      [-0.0263, -0.0028,  0.0834,  ...,  0.1057, -0.1417, -0.0102]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.4.mlp.mlp.2.bias',\n",
       "              tensor([-8.1704e-02,  8.2541e-05,  3.2078e-02,  1.6180e-03, -7.1967e-02,\n",
       "                       1.7817e-02,  5.1470e-02,  7.9083e-02, -8.5773e-02,  1.1046e-01,\n",
       "                       1.8952e-02, -3.5239e-02,  1.3363e-01,  1.1594e-02,  1.0147e-01,\n",
       "                       3.0846e-02], device='cuda:0')),\n",
       "             ('tab_layers.5.layer_norm1.weight',\n",
       "              tensor([1.0177, 1.0202, 0.9919, 1.0164, 0.9874, 0.9986, 1.0740, 1.0068, 1.0334,\n",
       "                      0.9986, 0.9919, 0.9790, 1.0226, 1.0142, 1.0104, 1.0346],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.layer_norm1.bias',\n",
       "              tensor([ 1.0688e-02,  3.0859e-03, -2.5108e-03,  1.4171e-02,  3.4310e-03,\n",
       "                       4.0376e-03,  4.0394e-05, -2.7307e-03, -1.5057e-02, -2.1790e-03,\n",
       "                      -3.9213e-03, -2.5743e-03,  1.7592e-03, -5.7462e-04, -1.7935e-02,\n",
       "                       2.0922e-03], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.query.weight',\n",
       "              tensor([[-0.1621, -0.2668,  0.2573, -0.5082,  0.2284, -0.3309, -0.1859,  0.1235,\n",
       "                        0.0189,  0.0960,  0.0808,  0.1076, -0.1972,  0.0695,  0.0964,  0.2446],\n",
       "                      [-0.3086, -0.1163,  0.2242,  0.0288, -0.1688,  0.1368,  0.4166, -0.1860,\n",
       "                        0.2227,  0.1511, -0.0611, -0.1157,  0.0508,  0.3589,  0.1297,  0.1449]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.query.bias',\n",
       "              tensor([0.1564, 0.2318], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.key.weight',\n",
       "              tensor([[-0.3054,  0.0132,  0.2575,  0.1315,  0.0044, -0.1191,  0.0582,  0.0031,\n",
       "                        0.3518, -0.2959, -0.1876,  0.3707, -0.1211,  0.2853, -0.2169,  0.3326],\n",
       "                      [-0.1137,  0.0823,  0.2698, -0.2285,  0.2246,  0.2524,  0.0590, -0.3118,\n",
       "                       -0.1271, -0.2920, -0.2214,  0.3048,  0.2589, -0.1125,  0.1390,  0.2563]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.key.bias',\n",
       "              tensor([0.1617, 0.0715], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.values.weight',\n",
       "              tensor([[ 0.0423,  0.1946,  0.0351,  0.1041, -0.0963,  0.0613,  0.1560, -0.1011,\n",
       "                       -0.1668, -0.2483,  0.0120,  0.0283,  0.1837, -0.1969,  0.1393,  0.1425],\n",
       "                      [ 0.0607, -0.1881, -0.1547,  0.1971, -0.1664, -0.2170,  0.0577,  0.0069,\n",
       "                        0.0471, -0.2035,  0.0235,  0.1883, -0.0122,  0.2275, -0.1287,  0.1643]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.0.values.bias',\n",
       "              tensor([0.0058, 0.0778], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.query.weight',\n",
       "              tensor([[ 0.0440,  0.0118,  0.0628, -0.3289,  0.2633, -0.1143, -0.1568,  0.1574,\n",
       "                       -0.0915,  0.1345, -0.1774,  0.0172,  0.3159,  0.1166,  0.0137,  0.1290],\n",
       "                      [-0.1151,  0.1317, -0.1633,  0.2246,  0.1072, -0.1071,  0.3016, -0.2267,\n",
       "                        0.0720, -0.2834, -0.1103, -0.0337,  0.0853, -0.1699, -0.1634,  0.0182]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.query.bias',\n",
       "              tensor([0.0336, 0.0341], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.key.weight',\n",
       "              tensor([[ 0.1779, -0.2065, -0.3676, -0.0835,  0.0276,  0.1030, -0.2105, -0.2250,\n",
       "                        0.2453, -0.1361, -0.2093, -0.0801, -0.2113, -0.4474, -0.2994,  0.1653],\n",
       "                      [ 0.0699, -0.4536,  0.0130, -0.1330,  0.0373, -0.1793, -0.2673, -0.0870,\n",
       "                       -0.2654,  0.2747,  0.0619, -0.1084,  0.0047,  0.0253,  0.1696,  0.0445]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.key.bias',\n",
       "              tensor([0.1803, 0.0971], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.values.weight',\n",
       "              tensor([[ 0.1178,  0.0924,  0.0283,  0.2219, -0.1651,  0.0302, -0.1721, -0.0309,\n",
       "                        0.2650,  0.2263,  0.0450,  0.2666, -0.1435, -0.0584,  0.1107, -0.1939],\n",
       "                      [ 0.0023, -0.0017, -0.0806,  0.1364, -0.2320, -0.1090, -0.1635, -0.0709,\n",
       "                        0.1112,  0.0072, -0.0315, -0.1612, -0.1924,  0.0961,  0.1191, -0.1437]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.1.values.bias',\n",
       "              tensor([0.0824, 0.1698], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.query.weight',\n",
       "              tensor([[ 0.3187, -0.1023, -0.2877,  0.3146,  0.2916,  0.3300, -0.1489,  0.0314,\n",
       "                       -0.0358, -0.3637,  0.0201,  0.4113, -0.1056, -0.2188, -0.1639,  0.0553],\n",
       "                      [-0.0438,  0.0153,  0.0206,  0.3442,  0.1044,  0.3978,  0.0965, -0.2948,\n",
       "                        0.3239,  0.0425, -0.2385,  0.2142,  0.1853, -0.1852, -0.0408, -0.1786]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.query.bias',\n",
       "              tensor([-0.2344, -0.0614], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.key.weight',\n",
       "              tensor([[-0.2426, -0.2391, -0.3510, -0.0484,  0.1641,  0.0270, -0.0602, -0.0840,\n",
       "                       -0.3316,  0.2997,  0.0602, -0.0813,  0.2456,  0.2492,  0.3461,  0.2158],\n",
       "                      [-0.2947, -0.4420,  0.0279, -0.1786,  0.1465,  0.0005,  0.2356,  0.2680,\n",
       "                       -0.3879,  0.0912,  0.2186, -0.0102,  0.0941,  0.3186,  0.4467,  0.1124]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.key.bias',\n",
       "              tensor([-0.2399,  0.2171], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.values.weight',\n",
       "              tensor([[ 0.1704, -0.0532,  0.0810, -0.0986,  0.0647,  0.1793,  0.2241,  0.1886,\n",
       "                       -0.0338, -0.0592,  0.0829, -0.1079, -0.2985, -0.1461, -0.1979, -0.0831],\n",
       "                      [ 0.1553, -0.0819, -0.2015,  0.1898,  0.0457,  0.2748, -0.1324, -0.3062,\n",
       "                        0.2259, -0.0119, -0.0822,  0.1396, -0.2322, -0.1053,  0.1715, -0.1894]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.2.values.bias',\n",
       "              tensor([0.0852, 0.1300], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.query.weight',\n",
       "              tensor([[ 0.4711,  0.1428, -0.3246,  0.2060, -0.3973,  0.3669,  0.2477, -0.0662,\n",
       "                       -0.0480, -0.2215,  0.0283,  0.1780,  0.0122, -0.3097, -0.3224, -0.1064],\n",
       "                      [-0.3320,  0.1078,  0.3345, -0.1976,  0.2160, -0.3079, -0.3963, -0.0147,\n",
       "                        0.0563, -0.0991, -0.0817, -0.2675, -0.0547,  0.3184,  0.0116,  0.0710]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.query.bias',\n",
       "              tensor([-0.3726,  0.2558], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.key.weight',\n",
       "              tensor([[-0.3609, -0.2281, -0.3556, -0.0141,  0.1093, -0.2709,  0.2127,  0.1275,\n",
       "                        0.1401, -0.1980,  0.0037,  0.1020,  0.3810,  0.3594,  0.0286, -0.1049],\n",
       "                      [-0.0492,  0.3317,  0.0186, -0.2324, -0.2338,  0.0702,  0.1902, -0.4458,\n",
       "                       -0.2627, -0.2719, -0.3872, -0.1314, -0.3583, -0.4469, -0.1214, -0.1839]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.key.bias',\n",
       "              tensor([-0.1793, -0.1248], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.values.weight',\n",
       "              tensor([[-0.0122,  0.0328,  0.0487,  0.0971,  0.1362, -0.2284, -0.0626,  0.2233,\n",
       "                       -0.1328, -0.1318,  0.0794,  0.0906, -0.1767,  0.0755, -0.2771,  0.1210],\n",
       "                      [ 0.2740,  0.1604,  0.0732,  0.1972,  0.2854,  0.1849,  0.1345, -0.0593,\n",
       "                       -0.1994, -0.2745, -0.0182, -0.1349, -0.2125, -0.1182,  0.0664, -0.2595]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.3.values.bias',\n",
       "              tensor([-0.0349,  0.1443], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.query.weight',\n",
       "              tensor([[ 0.0258,  0.2157, -0.5415,  0.2196, -0.1794, -0.1429,  0.2075,  0.2077,\n",
       "                       -0.1588, -0.0998, -0.0111,  0.1113, -0.0869,  0.1623, -0.2057,  0.0373],\n",
       "                      [ 0.0812, -0.1715, -0.0080,  0.0420, -0.3033, -0.3156, -0.0671,  0.1072,\n",
       "                        0.0570, -0.0994,  0.3058,  0.0544,  0.1191,  0.2061,  0.0208, -0.0854]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.query.bias',\n",
       "              tensor([-0.3137, -0.1690], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.key.weight',\n",
       "              tensor([[-0.1226, -0.1623,  0.1997, -0.0448, -0.0943, -0.1623,  0.1795,  0.4830,\n",
       "                       -0.2086,  0.0485, -0.0020, -0.0581, -0.1393,  0.3957,  0.0038, -0.0721],\n",
       "                      [-0.1164,  0.0837,  0.0441,  0.2430, -0.2304, -0.2229,  0.1945, -0.0882,\n",
       "                       -0.3055,  0.1845, -0.1259, -0.0284, -0.0334, -0.2819, -0.3071, -0.2358]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.key.bias',\n",
       "              tensor([0.0049, 0.0472], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.values.weight',\n",
       "              tensor([[-0.1713,  0.1642,  0.1275,  0.2303, -0.0380, -0.0501,  0.2036,  0.1497,\n",
       "                       -0.1581,  0.1200,  0.2255,  0.1987,  0.0683, -0.0910, -0.2899, -0.1868],\n",
       "                      [-0.1552, -0.0200, -0.1515, -0.1979,  0.0534, -0.1896, -0.1411, -0.1750,\n",
       "                        0.0197, -0.1030,  0.2132,  0.1005,  0.0029,  0.2257, -0.0529,  0.2133]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.4.values.bias',\n",
       "              tensor([-0.1833, -0.0158], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.query.weight',\n",
       "              tensor([[-0.0357,  0.1137, -0.0372,  0.1707,  0.0135,  0.2155,  0.3058, -0.0425,\n",
       "                       -0.0984, -0.0668,  0.1971, -0.0360, -0.2145,  0.1537, -0.1632,  0.0668],\n",
       "                      [-0.4795, -0.1387,  0.2676, -0.2561,  0.1452, -0.0096, -0.1905,  0.1973,\n",
       "                       -0.1002,  0.1123,  0.0302,  0.0276,  0.1847, -0.0195,  0.4490, -0.1511]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.query.bias',\n",
       "              tensor([-0.1443,  0.4257], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.key.weight',\n",
       "              tensor([[-0.1827,  0.1882,  0.1952, -0.2929, -0.0389,  0.1004, -0.2636, -0.0482,\n",
       "                       -0.2014, -0.1383,  0.1455,  0.1535, -0.1614, -0.1064, -0.0034, -0.1286],\n",
       "                      [-0.1619, -0.0373,  0.0941, -0.1667, -0.0436, -0.3902,  0.2098,  0.2255,\n",
       "                       -0.2177,  0.1510,  0.0855, -0.3635,  0.2748,  0.1339, -0.0040,  0.0446]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.key.bias',\n",
       "              tensor([-0.0136, -0.2159], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.values.weight',\n",
       "              tensor([[ 0.0347,  0.2449, -0.0360,  0.0942, -0.2653,  0.0047, -0.2324, -0.1650,\n",
       "                        0.0455, -0.0871, -0.2862,  0.1222,  0.0476, -0.0847,  0.1612, -0.1178],\n",
       "                      [-0.0253, -0.0151, -0.0893, -0.0652,  0.0393,  0.0466, -0.0385, -0.2199,\n",
       "                        0.0598,  0.2511,  0.2382, -0.0091, -0.1185,  0.1472,  0.2566,  0.0646]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.5.values.bias',\n",
       "              tensor([-0.2555,  0.2481], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.query.weight',\n",
       "              tensor([[-0.0044, -0.3743, -0.0733,  0.1126, -0.1803, -0.0095, -0.0656,  0.1020,\n",
       "                        0.1805,  0.2657,  0.3164,  0.1554, -0.2233,  0.3664,  0.1645,  0.1175],\n",
       "                      [-0.1559,  0.0525,  0.2075,  0.3211, -0.0589, -0.2249, -0.3920, -0.0417,\n",
       "                       -0.0171,  0.0302, -0.3438,  0.1847,  0.2062, -0.2379,  0.1867,  0.1482]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.query.bias',\n",
       "              tensor([ 0.2402, -0.0521], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.key.weight',\n",
       "              tensor([[ 0.1004,  0.1197,  0.2936, -0.1445,  0.1827,  0.3465, -0.2182,  0.0349,\n",
       "                        0.3230, -0.2823, -0.3707,  0.1343, -0.1729, -0.0656, -0.1670,  0.0783],\n",
       "                      [ 0.0983, -0.1283, -0.2599,  0.3703, -0.2015, -0.2610, -0.1947,  0.0733,\n",
       "                       -0.3901,  0.1345, -0.0520, -0.2160, -0.1418,  0.2786, -0.4573,  0.1555]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.key.bias',\n",
       "              tensor([-0.1979,  0.0379], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.values.weight',\n",
       "              tensor([[ 0.0589, -0.0802,  0.0755,  0.1725,  0.1812, -0.0713,  0.2173,  0.0768,\n",
       "                        0.2355,  0.2405, -0.2040, -0.1741, -0.1287,  0.0882, -0.2703, -0.1606],\n",
       "                      [ 0.0141,  0.1076,  0.0324,  0.0522, -0.1340,  0.1441, -0.1813, -0.0745,\n",
       "                       -0.0390, -0.2331, -0.0870, -0.1616,  0.2615,  0.2431, -0.1619,  0.1905]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.6.values.bias',\n",
       "              tensor([0.0362, 0.1409], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.query.weight',\n",
       "              tensor([[ 0.0187,  0.0934, -0.3166,  0.0886,  0.0165,  0.0544,  0.0343, -0.2197,\n",
       "                        0.2018,  0.0624,  0.1006, -0.1131,  0.2487,  0.0680, -0.1928, -0.0840],\n",
       "                      [ 0.3651,  0.4837, -0.3792,  0.4819,  0.5141,  0.2880,  0.0960, -0.0844,\n",
       "                       -0.0863, -0.4277,  0.0518,  0.0376,  0.3215, -0.0664, -0.2794,  0.2402]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.query.bias',\n",
       "              tensor([ 0.0823, -0.3849], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.key.weight',\n",
       "              tensor([[-0.1680,  0.0202, -0.0137, -0.1348,  0.1932, -0.2994,  0.1059,  0.1005,\n",
       "                       -0.0404,  0.1026,  0.2213, -0.2253, -0.1304,  0.2114,  0.1286,  0.0535],\n",
       "                      [-0.1111, -0.4749, -0.3299,  0.2069,  0.1407, -0.2927, -0.2886,  0.4848,\n",
       "                        0.1108,  0.3425, -0.0886, -0.3402,  0.4012,  0.4038,  0.1632, -0.0356]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.key.bias',\n",
       "              tensor([-0.0885,  0.0517], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.values.weight',\n",
       "              tensor([[ 0.2415, -0.1959, -0.1652, -0.1042,  0.0397,  0.1357,  0.2180,  0.1762,\n",
       "                        0.0119, -0.0901, -0.0241,  0.1992, -0.3280,  0.1120, -0.0599, -0.1775],\n",
       "                      [ 0.2238, -0.1627, -0.1153,  0.0955,  0.0470,  0.0248, -0.2069,  0.0067,\n",
       "                       -0.1181, -0.1837,  0.1196,  0.1410, -0.1869, -0.1980,  0.1797,  0.1178]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.heads.7.values.bias',\n",
       "              tensor([0.0248, 0.0698], device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.proj.weight',\n",
       "              tensor([[ 0.0964, -0.0667, -0.1801,  0.0686, -0.2632, -0.0252,  0.0697,  0.0633,\n",
       "                       -0.3529,  0.1399, -0.0568,  0.1588, -0.2002,  0.1026,  0.0242, -0.0616],\n",
       "                      [-0.1499,  0.0944,  0.2100,  0.1406,  0.2597,  0.2176, -0.2052, -0.2107,\n",
       "                       -0.1718, -0.0983, -0.0387, -0.0390, -0.1156, -0.2425,  0.1946,  0.1246],\n",
       "                      [-0.1001,  0.2521, -0.0246, -0.0345, -0.0784,  0.2136,  0.2395,  0.1582,\n",
       "                       -0.0122,  0.0050, -0.1881, -0.1761,  0.0022,  0.1362, -0.0778, -0.0525],\n",
       "                      [-0.1816,  0.1792,  0.0330, -0.1715,  0.0987, -0.1074,  0.1774, -0.1324,\n",
       "                        0.0484, -0.0893, -0.1876,  0.1152,  0.1975, -0.0412, -0.1875, -0.2294],\n",
       "                      [ 0.1899, -0.0345, -0.0773,  0.2390, -0.0213, -0.2230, -0.1146, -0.1010,\n",
       "                        0.0123,  0.1535,  0.1003,  0.1415,  0.2044, -0.1837,  0.2585, -0.0444],\n",
       "                      [-0.1160,  0.0416, -0.1285,  0.0520,  0.0076, -0.0467, -0.0973, -0.1115,\n",
       "                        0.0956,  0.0738,  0.1069, -0.1656, -0.1537,  0.1637,  0.0066,  0.1092],\n",
       "                      [ 0.0863,  0.1269,  0.1231,  0.1969, -0.0500, -0.2764, -0.2304,  0.1511,\n",
       "                       -0.0704, -0.0429, -0.2231,  0.0121,  0.0479, -0.0631, -0.3293, -0.1053],\n",
       "                      [ 0.2622,  0.0124, -0.0423, -0.1157,  0.0233,  0.0756, -0.0366, -0.0999,\n",
       "                       -0.0676,  0.1349,  0.0257, -0.2449, -0.2332,  0.2235,  0.0438, -0.2655],\n",
       "                      [-0.2179,  0.2151,  0.0808, -0.1574,  0.0568, -0.1085, -0.1540,  0.1160,\n",
       "                       -0.2475,  0.2198,  0.1489,  0.2019, -0.2556, -0.1531, -0.1059,  0.0865],\n",
       "                      [-0.1622,  0.2472, -0.1703, -0.0689,  0.3657, -0.0560, -0.1127,  0.1164,\n",
       "                        0.1677,  0.0921, -0.0476, -0.1764, -0.0913, -0.2390,  0.0907, -0.1400],\n",
       "                      [ 0.0732,  0.1008, -0.2413, -0.1854, -0.3591, -0.2797, -0.1101,  0.2245,\n",
       "                        0.1051, -0.0470, -0.0185, -0.2361, -0.2238, -0.1137,  0.0430, -0.3391],\n",
       "                      [-0.2334, -0.2570,  0.0587, -0.0310, -0.3739, -0.0923, -0.1113, -0.0958,\n",
       "                       -0.0302,  0.2503, -0.0571, -0.1107, -0.1457, -0.1815, -0.1870,  0.0048],\n",
       "                      [-0.0023, -0.0164,  0.1034,  0.0158,  0.1562, -0.1770,  0.0575, -0.1819,\n",
       "                        0.0353,  0.1933, -0.1784,  0.0493,  0.3016, -0.0427,  0.1971, -0.1473],\n",
       "                      [ 0.1631, -0.2528,  0.2140, -0.2481, -0.0495,  0.1679, -0.1500, -0.1034,\n",
       "                        0.1537, -0.0192,  0.1000, -0.1370, -0.2414, -0.0176, -0.2516,  0.1144],\n",
       "                      [ 0.1663,  0.1295, -0.0688, -0.1530,  0.0724, -0.0930,  0.1102,  0.0534,\n",
       "                       -0.1196,  0.1705,  0.1431,  0.0050,  0.2348, -0.1928, -0.1281, -0.2119],\n",
       "                      [-0.0046, -0.0931,  0.0740,  0.0981,  0.1243,  0.0767,  0.0010,  0.2524,\n",
       "                        0.0727, -0.2523,  0.2300,  0.2045, -0.1567, -0.1247,  0.2833, -0.1035]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.self_attention.proj.bias',\n",
       "              tensor([ 0.0431,  0.1441, -0.1442,  0.2535,  0.2329,  0.0197,  0.0865, -0.0562,\n",
       "                       0.1940, -0.1515,  0.1688,  0.2452,  0.2361, -0.1411,  0.0314,  0.0600],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.layer_norm2.weight',\n",
       "              tensor([1.0129, 1.0147, 1.0021, 1.0125, 1.0474, 0.9926, 1.0185, 1.0216, 0.9951,\n",
       "                      1.0030, 1.0113, 0.9914, 0.9975, 1.0131, 0.9968, 1.0394],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.layer_norm2.bias',\n",
       "              tensor([ 0.0066, -0.0020, -0.0039,  0.0066, -0.0077,  0.0041,  0.0044,  0.0002,\n",
       "                       0.0023, -0.0048,  0.0025,  0.0037, -0.0027,  0.0034, -0.0044, -0.0051],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.mlp.mlp.0.weight',\n",
       "              tensor([[-0.1802, -0.1759,  0.2175,  ...,  0.1303,  0.1426,  0.0059],\n",
       "                      [ 0.0212, -0.0092, -0.1889,  ...,  0.1246,  0.2340,  0.1691],\n",
       "                      [ 0.1590, -0.1091,  0.2503,  ..., -0.1197, -0.1344,  0.0141],\n",
       "                      ...,\n",
       "                      [-0.1320, -0.1355, -0.1962,  ...,  0.2019, -0.1748,  0.0946],\n",
       "                      [ 0.0610, -0.0037,  0.0560,  ...,  0.0419, -0.0636,  0.1060],\n",
       "                      [ 0.0046,  0.0167, -0.0189,  ..., -0.1703, -0.0975, -0.2792]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.mlp.mlp.0.bias',\n",
       "              tensor([-0.0918,  0.0636,  0.2347, -0.1613,  0.0270,  0.0976, -0.0212,  0.2465,\n",
       "                       0.2147, -0.1884,  0.0073,  0.1549,  0.0332, -0.2357, -0.0198, -0.2347,\n",
       "                      -0.1060, -0.1416,  0.1773, -0.1291, -0.2027,  0.1375, -0.0660, -0.0286,\n",
       "                       0.1747,  0.2249, -0.0165,  0.1782,  0.1470,  0.1948, -0.2256,  0.2059,\n",
       "                       0.0210,  0.0345, -0.0364,  0.0381,  0.2082,  0.0397, -0.0090, -0.1319,\n",
       "                      -0.1898, -0.1598,  0.0776,  0.0665, -0.0038, -0.0929,  0.0258, -0.0828,\n",
       "                       0.2105,  0.0217,  0.1611, -0.1763,  0.0111, -0.1450,  0.0692,  0.0656,\n",
       "                      -0.0483,  0.1097,  0.2035, -0.0902,  0.1514,  0.0228,  0.0896, -0.0854],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.mlp.mlp.2.weight',\n",
       "              tensor([[ 0.0869, -0.0586,  0.0718,  ..., -0.0096, -0.0021, -0.1116],\n",
       "                      [-0.0648, -0.0018, -0.0382,  ..., -0.0073, -0.0607,  0.0397],\n",
       "                      [-0.1115,  0.0663, -0.0648,  ...,  0.0285,  0.0933,  0.0039],\n",
       "                      ...,\n",
       "                      [ 0.1121, -0.0286, -0.0843,  ..., -0.0740, -0.0734,  0.0792],\n",
       "                      [-0.0038,  0.0144, -0.0798,  ..., -0.0669,  0.0239,  0.0922],\n",
       "                      [-0.1063, -0.1208,  0.0737,  ...,  0.0525, -0.0810,  0.0478]],\n",
       "                     device='cuda:0')),\n",
       "             ('tab_layers.5.mlp.mlp.2.bias',\n",
       "              tensor([ 0.0742, -0.0997, -0.0732, -0.0829, -0.0812,  0.0619, -0.1120,  0.1022,\n",
       "                       0.1150,  0.0136, -0.1180, -0.0227,  0.0155, -0.0680,  0.0092, -0.0671],\n",
       "                     device='cuda:0')),\n",
       "             ('unembedding.weight',\n",
       "              tensor([[ 0.1023, -0.1302, -0.1818,  0.1033, -0.0401,  0.1655,  0.0868,  0.0961,\n",
       "                        0.0635, -0.1240,  0.1444,  0.0713, -0.1184,  0.1427, -0.1453, -0.1976]],\n",
       "                     device='cuda:0')),\n",
       "             ('unembedding.bias', tensor([-0.1014], device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
